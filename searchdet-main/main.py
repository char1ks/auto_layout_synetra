#!/usr/bin/env python3
import os
import time
import argparse
import numpy as np
from PIL import Image

from mask_withsearch import (
    initialize_models,
    get_vector,
    adjust_embedding,
    annotate_image,
    annotate_image_v2,
)


def load_images_from_folder(folder: str):
    """Load all JPG/PNG images from a folder into a list of PIL.Image."""
    imgs = []
    for fn in sorted(os.listdir(folder)):
        if fn.lower().endswith((".jpg", ".jpeg", ".png")):
            path = os.path.join(folder, fn)
            imgs.append(Image.open(path).convert("RGB"))
    return imgs


def main():
    total_main_start = time.time()
    print("üöÄ –ó–∞–ø—É—Å–∫ SearchDet main pipeline...")
    
    parser = argparse.ArgumentParser(
        description="Run SearchDet on a single image using local positive/negative exemplars"
    )
    parser.add_argument(
        "--example", "-i",
        required=True,
        help="Path to the target image you want annotated",
    )
    parser.add_argument(
        "--positive-dir", "-p",
        required=True,
        help="Folder of positive support images",
    )
    parser.add_argument(
        "--negative-dir", "-n",
        required=True,
        help="Folder of negative support images",
    )
    parser.add_argument(
        "--output", "-o",
        default="annotated_output.jpg",
        help="Where to save the annotated result",
    )
    parser.add_argument(
        "--conf-threshold", "-c",
        type=float,
        default=0.5,
        help="Minimum confidence score to keep a detection (default: 0.3)",
    )
    parser.add_argument(
        "--iou-threshold", "-u",
        type=float,
        default=0.5,
        help="IoU threshold for non-maximum suppression (default: 0.5)",
    )
    args = parser.parse_args()

    # 1) Init all models (ResNet, transforms, SAM)
    init_start = time.time()
    print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π...")
    resnet_model, pooling_layer, transform, sam = initialize_models()
    init_time = time.time() - init_start
    print(f"   ‚è±Ô∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞: {init_time:.3f} —Å–µ–∫")

    # 2) Load images
    load_start = time.time()
    print("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
    example_img = Image.open(args.example).convert("RGB")
    positive_imgs = load_images_from_folder(args.positive_dir)
    negative_imgs = load_images_from_folder(args.negative_dir)
    load_time = time.time() - load_start
    print(f"   ‚è±Ô∏è –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(positive_imgs)} –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –∏ {len(negative_imgs)} –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞: {load_time:.3f} —Å–µ–∫")

    if len(positive_imgs) == 0 or len(negative_imgs) == 0:
        raise ValueError("Positive and negative folders must each contain ‚â•1 image.")

    # 3) Embed all support images
    embed_start = time.time()
    print("üß† –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø—Ä–∏–º–µ—Ä–æ–≤...")
    pos_embs = np.stack([
        get_vector(img, resnet_model, pooling_layer, transform).numpy()
        for img in positive_imgs
    ], axis=0).astype(np.float32)

    neg_embs = np.stack([
        get_vector(img, resnet_model, pooling_layer, transform).numpy()
        for img in negative_imgs
    ], axis=0).astype(np.float32)
    embed_time = time.time() - embed_start
    print(f"   ‚è±Ô∏è –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã –∑–∞: {embed_time:.3f} —Å–µ–∫")

    # 4) Compute adjusted query vectors
    adjust_start = time.time()
    print("‚öñÔ∏è –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ query –≤–µ–∫—Ç–æ—Ä–æ–≤...")
    adjusted_queries = np.stack([
        adjust_embedding(q, pos_embs, neg_embs)
        for q in pos_embs
    ], axis=0).astype(np.float32)
    adjust_time = time.time() - adjust_start
    print(f"   ‚è±Ô∏è Query –≤–µ–∫—Ç–æ—Ä—ã —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω—ã –∑–∞: {adjust_time:.3f} —Å–µ–∫")

    # 5) Annotate with thresholds
    annotation_start = time.time()
    print("üé® –ó–∞–ø—É—Å–∫ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
    annotate_image_v2(
        example_img,
        adjusted_queries,
        resnet_model,
        pooling_layer,
        transform,
        sam,
        args.output,
        iou_threshold=args.iou_threshold,
        min_area_threshold=6000,
        max_area_threshold=15000,
        similarity_thrsh=args.conf_threshold,
    )
    annotation_time = time.time() - annotation_start
    
    total_main_time = time.time() - total_main_start
    print(f"‚úÖ Annotated image written to {args.output}")
    print(f"\nüéâ –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –í–†–ï–ú–ï–ù–ò:")
    print(f"   üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π: {init_time:.3f}—Å ({init_time/total_main_time*100:.1f}%)")
    print(f"   üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {load_time:.3f}—Å ({load_time/total_main_time*100:.1f}%)")
    print(f"   üß† –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {embed_time:.3f}—Å ({embed_time/total_main_time*100:.1f}%)")
    print(f"   ‚öñÔ∏è –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ query: {adjust_time:.3f}—Å ({adjust_time/total_main_time*100:.1f}%)")
    print(f"   üé® –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {annotation_time:.3f}—Å ({annotation_time/total_main_time*100:.1f}%)")
    print(f"üìä –û–ë–©–ï–ï –í–†–ï–ú–Ø: {total_main_time:.3f} —Å–µ–∫")


if __name__ == "__main__":
    main()
