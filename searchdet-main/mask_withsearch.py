import os
import torch
import torchvision.transforms as transforms
import torchvision.models as models
from PIL import Image
import numpy as np
import cv2
import faiss
from sklearn.metrics.pairwise import cosine_similarity
import hashlib
import pickle
from segment_anything_hq import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor
import timm
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time
import base64
from io import BytesIO
import requests
from langchain_community.llms import VLLM
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate
import urllib.request
from pathlib import Path


def initialize_sam():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è SearchDet: ResNet, —Å–ª–æ–π, —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º –∏ SAM"""
    total_init_start = time.time()
    print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π SearchDet...")
    
    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ResNet –º–æ–¥–µ–ª–∏
    resnet_start = time.time()
    print("üß† –ó–∞–≥—Ä—É–∑–∫–∞ ResNet101 –º–æ–¥–µ–ª–∏...")
    resnet_model = models.resnet101(pretrained=True)
    resnet_model.eval()
    
    # –ü–µ—Ä–µ–≤–æ–¥ –Ω–∞ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    resnet_model = resnet_model.to(device)
    
    # –í—ã–±–∏—Ä–∞–µ–º —Å–ª–æ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–∏—á (layer3 –¥–ª—è —Ö–æ—Ä–æ—à–µ–≥–æ –±–∞–ª–∞–Ω—Å–∞)
    pooling_layer = resnet_model.layer3
    
    resnet_time = time.time() - resnet_start
    print(f"   ‚è±Ô∏è ResNet101 –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞: {resnet_time:.3f} —Å–µ–∫")
    
    # 2. –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞ –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
    transform_start = time.time()
    print("üîß –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞...")
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    transform_time = time.time() - transform_start
    print(f"   ‚è±Ô∏è –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º —Å–æ–∑–¥–∞–Ω –∑–∞: {transform_time:.3f} —Å–µ–∫")
    
    # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SAM –º–æ–¥–µ–ª–∏
    sam_start = time.time()
    print("üéØ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SAM-HQ –º–æ–¥–µ–ª–∏...")
    
    # –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    sam_checkpoint = download_sam_hq_model()
    
    if sam_checkpoint is None:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å SAM-HQ, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
        # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
        sam = None
    else:
        try:
            model_type = "vit_l"
            sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
            sam.to(device=device)
            print(f"‚úÖ SAM-HQ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {sam_checkpoint}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ SAM-HQ: {e}")
            sam = None
    
    sam_time = time.time() - sam_start
    print(f"   ‚è±Ô∏è SAM –º–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –∑–∞: {sam_time:.3f} —Å–µ–∫")
    
    total_init_time = time.time() - total_init_start
    print(f"üéâ –û–±—â–µ–µ –≤—Ä–µ–º—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {total_init_time:.3f} —Å–µ–∫")
    
    return resnet_model, pooling_layer, transform, sam


def initialize_models():
    """–ê–ª–∏–∞—Å –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    return initialize_sam()


def download_sam_hq_model():
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ SAM-HQ –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç"""
    sam_dir = Path("sam-hq/pretrained_checkpoint")
    sam_checkpoint = sam_dir / "sam_hq_vit_l.pth"
    
    if sam_checkpoint.exists():
        print(f"‚úÖ SAM-HQ –º–æ–¥–µ–ª—å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {sam_checkpoint}")
        return str(sam_checkpoint)
    
    print("üì• SAM-HQ –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, —Å–∫–∞—á–∏–≤–∞–µ–º...")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    sam_dir.mkdir(parents=True, exist_ok=True)
    
    # URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è SAM-HQ –º–æ–¥–µ–ª–∏
    model_url = "https://huggingface.co/lkeab/hq-sam/resolve/main/sam_hq_vit_l.pth"
    
    try:
        print(f"üîÑ –°–∫–∞—á–∏–≤–∞–µ–º SAM-HQ –º–æ–¥–µ–ª—å –∏–∑ {model_url}")
        print("‚ö†Ô∏è –≠—Ç–æ –∑–∞–π–º–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç (~1.2GB)...")
        
        # –°–∫–∞—á–∏–≤–∞–µ–º —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        def reporthook(blocknum, blocksize, totalsize):
            if totalsize > 0:
                percent = min(blocknum * blocksize * 100 / totalsize, 100)
                print(f"\rüì• –ü—Ä–æ–≥—Ä–µ—Å—Å: {percent:.1f}%", end="", flush=True)
        
        urllib.request.urlretrieve(model_url, sam_checkpoint, reporthook=reporthook)
        print(f"\n‚úÖ SAM-HQ –º–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞: {sam_checkpoint}")
        
        return str(sam_checkpoint)
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è SAM-HQ: {e}")
        print("üîÑ –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π URL...")
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π URL
        alt_url = "https://github.com/SysCV/sam-hq/releases/download/v0.3/sam_hq_vit_l.pth"
        
        try:
            urllib.request.urlretrieve(alt_url, sam_checkpoint, reporthook=reporthook)
            print(f"\n‚úÖ SAM-HQ –º–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞ (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫): {sam_checkpoint}")
            return str(sam_checkpoint)
            
        except Exception as e2:
            print(f"\n‚ùå –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {e2}")
            print("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –Ω–∞ –æ–±—ã—á–Ω—É—é SAM –º–æ–¥–µ–ª—å...")
            return None


def get_conversation_chain():
    llm = VLLM(
        model="microsoft/Phi-3-mini-4k-instruct",
        trust_remote_code=True,
        max_new_tokens=10,
        top_k=20,
        top_p=0.8,
        temperature=0.8,
        dtype="float16",
        tensor_parallel_size=8
    )

    prompt_template = PromptTemplate(
        input_variables=["question"],
        template="""
        Given a primary visual concept, provide a list of negative concepts that would commonly appear in the same visual setting. 
        These negative concepts should visually interfere or hinder the localization and clear identification of the primary concept. 
        For each example, avoid objects identical or too similar to the main concept but focus on those that share the context or background. 
        For example: Primary Concept: 'Surfboard' Negative Concepts: 'Waves', 'Sand' Primary Concept: 'Fork' Negative Concepts: 'Plates', 'Food'.
        Now, for the following list of primary concepts, generate similar lists of negative concepts. conceptlist: []

        Question: {question}
        
        Response:
        """
    )

    # Create the LLMChain
    conversation_chain = LLMChain(
        llm=llm,
        prompt=prompt_template
    )

    return conversation_chain


def get_negative_word(positive_word):
    chain = get_conversation_chain()
    template_argument_identification = f"What is the opposite of {positive_word}?"
    response = chain.run(question=template_argument_identification).strip().lower()
    print(f"Negative word for '{positive_word}' is '{response}'")
    return response


# Function to scrape images from Google Images
def scrape_images(keyword, save_dir, num_images=5):
    options = Options()
    options.add_argument('--no-sandbox')
    options.add_argument('--headless')
    options.add_argument('--disable-dev-shm-usage')
    driver = webdriver.Chrome(options=options)

    driver.get('https://images.google.com/')
    search_box = driver.find_element(By.NAME, 'q')
    search_box.send_keys(keyword)
    search_box.submit()
    time.sleep(3)  # Wait for the page to load

    os.makedirs(save_dir, exist_ok=True)

    def save_image(img_url, img_name):
        if img_url.startswith('data:image/jpeg;base64,') or img_url.startswith('data:image/png;base64,'):
            img_data = base64.b64decode(img_url.split(',')[1])
            img = Image.open(BytesIO(img_data))
            img.save(os.path.join(save_dir, img_name))
        else:
            response = requests.get(img_url)
            with open(os.path.join(save_dir, img_name), 'wb') as f:
                f.write(response.content)

    for i in range(num_images):
        try:
            img_xpath = f'//div[@jsname="dTDiAc"][{i+1}]//img'
            img_tag = driver.find_element(By.XPATH, img_xpath)
            img_url = img_tag.get_attribute('src')
            save_image(img_url, f'image_{i + 1}.jpg')
            print("Downloaded image:", f'image_{i + 1}.jpg')
        except Exception as e:
            print(f"Could not download image {i + 1}: {e}")

    driver.quit()




# Function to extract features from an image using DINOv2 or ResNet
def get_vector(image, model, layer, transform):
    t_img = transform(image).unsqueeze(0)
    
    # ---> FIX: move tensor to model's device
    try:
        device = next(model.parameters()).device
        t_img = t_img.to(device)
    except Exception as e:
        print(f"[get_vector] Could not move tensor to device: {e}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–æ–¥–µ–ª—å DINOv2/v3
    model_name = model.__class__.__name__.lower()
    if 'vit' in model_name or hasattr(model, 'forward_features'):
        # DINOv2/v3 –º–æ–¥–µ–ª—å - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π –≤—ã–∑–æ–≤
        with torch.no_grad():
            if hasattr(model, 'forward_features'):
                # DINOv2/v3: —Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å ‚Äî —ç—Ç–æ timm-–º–æ–¥–µ–ª—å
                feats = model.forward_features(t_img)
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã—Ö–æ–¥–∞: –º–æ–∂–µ—Ç –±—ã—Ç—å dict –∏–ª–∏ tensor
                if isinstance(feats, dict):
                    if 'x_norm_clstoken' in feats: 
                        vec = feats['x_norm_clstoken'][0]
                    elif 'x_norm_patchtokens' in feats: 
                        vec = feats['x_norm_patchtokens'][0].mean(dim=0)
                    else: 
                        vec = next(iter(feats.values()))[0].mean(dim=0)
                elif torch.is_tensor(feats):
                    if feats.ndim == 3 and feats.shape[1] > 0: 
                        vec = feats[0, 0] # CLS token for B=1
                    else: 
                        vec = feats.flatten() # Fallback to flatten
                else:
                    raise TypeError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø –≤—ã—Ö–æ–¥–∞ DINOv2/v3: {type(feats)}")
                
                # L2 –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                temp_embedding = torch.nn.functional.normalize(vec.float(), p=2, dim=0)
            else:
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π forward –¥–ª—è –¥—Ä—É–≥–∏—Ö ViT –º–æ–¥–µ–ª–µ–π
                output = model(t_img)
                temp_embedding = output.flatten()
    else:
        # ResNet –º–æ–¥–µ–ª—å - –∏—Å–ø–æ–ª—å–∑—É–µ–º hook –Ω–∞ —Å–ª–æ–π
        temp_embedding = None

        def copy_data(m, i, o):
            nonlocal temp_embedding
            # –ë–µ—Ä–µ–º feature map –∏ –¥–µ–ª–∞–µ–º Global Average Pooling –∫–∞–∫ –≤ –±—ã—Å—Ç—Ä–æ–º –º–µ—Ç–æ–¥–µ
            if len(o.shape) == 4:  # [B, C, H, W]
                pooled = torch.nn.functional.adaptive_avg_pool2d(o, (1, 1))
                temp_embedding = pooled.flatten()
            else:
                temp_embedding = o.flatten()

        # –£–≤–∞–∂–∞–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π layer: —Å—Ç—Ä–æ–∫–∞ ('layer2', 'layer3', ...) –∏–ª–∏ –º–æ–¥—É–ª—å
        target_layer = None
        if isinstance(layer, str) and hasattr(model, layer):
            blk = getattr(model, layer)
            target_layer = blk[-1] if isinstance(blk, torch.nn.Sequential) else blk
        elif isinstance(layer, torch.nn.Module):
            target_layer = layer
        else:
            # Fallback: —Å—Ç–∞—Ä—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏
            if hasattr(model, 'layer3'):
                target_layer = model.layer3[-1]
            elif hasattr(model, 'layer4'):
                target_layer = model.layer4[-1]
            else:
                target_layer = layer
        
        h = target_layer.register_forward_hook(copy_data)
        with torch.no_grad():
            model(t_img)
        h.remove()
    
    if temp_embedding is None:
        # Fallback - —Å–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        temp_embedding = torch.zeros(1024)
    
    return temp_embedding


# üî• –ö–≠–®–ò–†–û–í–ê–ù–ò–ï –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–∏–º–µ—Ä–æ–≤
def get_cached_embeddings(image_paths, model, layer, transform, cache_dir="cache"):
    """
    –ö—ç—à–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –¥–∏—Å–∫ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
    """
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir)
    
    # –°–æ–∑–¥–∞–µ–º —Ö—ç—à –æ—Ç –ø—É—Ç–µ–π —Ñ–∞–π–ª–æ–≤ –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∏–º–µ–Ω–∏ –∫—ç—à–∞
    paths_str = "|".join(sorted(image_paths))
    cache_hash = hashlib.md5(paths_str.encode()).hexdigest()
    cache_file = os.path.join(cache_dir, f"embeddings_{cache_hash}.pkl")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∫—ç—à –∏ –∞–∫—Ç—É–∞–ª–µ–Ω –ª–∏ –æ–Ω
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'rb') as f:
                cached_data = pickle.load(f)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —Ñ–∞–π–ª—ã –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å
            cache_valid = True
            for path, cached_mtime in cached_data['mtimes'].items():
                if not os.path.exists(path) or os.path.getmtime(path) != cached_mtime:
                    cache_valid = False
                    break
            
            if cache_valid:
                print(f"   üì¶ –ó–∞–≥—Ä—É–∂–µ–Ω –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ({len(cached_data['embeddings'])} —Ñ–∞–π–ª–æ–≤)")
                return cached_data['embeddings']
        except:
            pass  # –ö—ç—à –ø–æ–≤—Ä–µ–∂–¥–µ–Ω, –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º
    
    # –í—ã—á–∏—Å–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∑–∞–Ω–æ–≤–æ
    embeddings = []
    mtimes = {}
    
    print(f"   üîß –°–æ–∑–¥–∞–µ–º –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(image_paths)} —Ñ–∞–π–ª–æ–≤...")
    for path in image_paths:
        if os.path.exists(path):
            image = Image.open(path).convert('RGB')
            embedding = get_vector(image, model, layer, transform).cpu().numpy()
            embeddings.append(embedding)
            mtimes[path] = os.path.getmtime(path)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
    cache_data = {
        'embeddings': np.array(embeddings),
        'mtimes': mtimes
    }
    
    try:
        with open(cache_file, 'wb') as f:
            pickle.dump(cache_data, f)
        print(f"   üíæ –ö—ç—à —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {cache_file}")
    except:
        print(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à")
    
    return np.array(embeddings)


# üöÄ FAST Function to extract features using Masked Pooling (10-50x faster!)
def extract_features_from_masks_fast(image, masks, model, layer, transform):
    """
    üöÄ –ë–´–°–¢–†–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ features —Å Masked Pooling - –æ–¥–∏–Ω –ø—Ä–æ–≥–æ–Ω –±—ç–∫–±–æ–Ω–∞ –Ω–∞ –≤—Å–µ –º–∞—Å–∫–∏!
    –£—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ 10-50 —Ä–∞–∑ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ –ø—Ä–æ–≥–æ–Ω–∞–º–∏ –∫–∞–∂–¥–æ–π –º–∞—Å–∫–∏.
    """
    import time
    import torch
    import torch.nn.functional as F
    
    extract_start = time.time()
    print(f"üöÄ –ë–´–°–¢–†–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(masks)} –º–∞—Å–æ–∫ (Masked Pooling)...")
    
    if len(masks) == 0:
        print("   ‚ùå –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ü–µ—Ä–µ–¥–∞–Ω–æ 0 –º–∞—Å–æ–∫ –≤ —Ñ—É–Ω–∫—Ü–∏—é!")
        return np.array([])
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏
    if transform:
        image_tensor = transform(Image.fromarray(image)).unsqueeze(0)
    else:
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        image_tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
        # ImageNet –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        image_tensor = (image_tensor - mean) / std
        image_tensor = image_tensor.unsqueeze(0)
    
    # ---> FIX: move tensor to model's device and match dtype
    device = next(model.parameters()).device
    image_tensor = image_tensor.to(device)
    
    # Match model dtype (e.g. half)
    if next(model.parameters()).dtype == torch.float16:
        image_tensor = image_tensor.half()
    
    # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞
    print(f"   üß™ Backbone input tensor: {list(image_tensor.shape)}")  # –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å [1,3,?,?] –≥–¥–µ ? ~512
    print(f"   üß™ SEARCHDET_FEAT_SHORT_SIDE = {os.getenv('SEARCHDET_FEAT_SHORT_SIDE', '–Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ')}")
    
    # üî• Hook –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ feature map –ò–ó –ü–†–ê–í–ò–õ–¨–ù–û–ì–û –°–õ–û–Ø
    feature_map = None
    def hook_fn(module, input, output):
        nonlocal feature_map
        # –ü–æ–ª—É—á–∞–µ–º feature map –ø–µ—Ä–µ–¥ Global Average Pool
        if len(output.shape) == 4:  # [B, C, H, W] - —ç—Ç–æ —Ç–æ —á—Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ
            feature_map = output
    
    # –ò—â–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Å–ª–æ–π –¥–ª—è hook (–Ω–µ avgpool/fc).
    # ‚ö†Ô∏è –£–≤–∞–∂–∞—è –∞—Ä–≥—É–º–µ–Ω—Ç `layer`: –¥–æ–ø—É—Å—Ç–∏–º—ã 'layer1'/'layer2'/'layer3'/'layer4'.
    target_layer = None
    # 1) –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–ª–∏ —Å—Ç—Ä–æ–∫–æ–≤—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –±–ª–æ–∫–∞ ResNet ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
    if isinstance(layer, str) and hasattr(model, layer):
        blk = getattr(model, layer)
        target_layer = blk[-1] if isinstance(blk, torch.nn.Sequential) else blk
        print(f"   üîß –í—ã–±—Ä–∞–Ω —Å–ª–æ–π –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–∞ --layer: {layer}")
    # 2) –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–ª–∏ —Å–∞–º –º–æ–¥—É–ª—å ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
    elif isinstance(layer, torch.nn.Module):
        target_layer = layer
    # 3) –ò–Ω–∞—á–µ ‚Äî —Å—Ç–∞—Ä—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏
    elif hasattr(model, 'layer4'):  # ResNet style
        if hasattr(model, 'layer2'):
            target_layer = model.layer2[-1]
            print("   üîß –ê–≤—Ç–æ–≤—ã–±–æ—Ä: layer2 (stride=8) –¥–ª—è –±–æ–ª–µ–µ –ø–ª–æ—Ç–Ω–æ–≥–æ feature map")
        elif hasattr(model, 'layer3'):
            target_layer = model.layer3[-1]
            print("   üîß –ê–≤—Ç–æ–≤—ã–±–æ—Ä: layer3")
        else:
            target_layer = model.layer4[-1]
    elif hasattr(model, 'features'):  # DenseNet/VGG style
        target_layer = model.features[-3] if len(model.features) > 3 else model.features[-1]
    elif hasattr(model, 'classifier') and hasattr(model, 'features'):
        target_layer = model.features[-3] if len(model.features) > 3 else model.features[-2]
    else:
        # Fallback ‚Äî —Ä–∞–Ω–Ω–∏–π conv
        conv_layers = []
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Conv2d):
                conv_layers.append((name, module))
        if len(conv_layers) >= 3:
            target_layer = conv_layers[-3][1]
            print(f"   üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º conv —Å–ª–æ–π: {conv_layers[-3][0]}")
        elif conv_layers:
            target_layer = conv_layers[-1][1]
    
    if target_layer is None:
        print("   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Å–ª–æ–π –¥–ª—è hook, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥")
        return extract_features_from_masks_slow(image, masks, model, layer, transform)
    
    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º hook –Ω–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã–π —Å–ª–æ–π
    hook = target_layer.register_forward_hook(hook_fn)
    
    with torch.no_grad():
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä—Å–∏—é PyTorch –¥–ª—è autocast
        try:
            with torch.amp.autocast('cuda'):
                # üî• –û–î–ò–ù –ø—Ä–æ–≥–æ–Ω –±—ç–∫–±–æ–Ω–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
                model.eval()
                if hasattr(model, 'to'):
                    model = model.to(memory_format=torch.channels_last)
                _ = model(image_tensor)
        except (AttributeError, TypeError):
            # Fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π PyTorch
            try:
                with torch.cuda.amp.autocast():
                    model.eval()
                    if hasattr(model, 'to'):
                        model = model.to(memory_format=torch.channels_last)
                    _ = model(image_tensor)
            except:
                # –ë–µ–∑ autocast –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
                model.eval()
                _ = model(image_tensor)
    
    hook.remove()
    
    if feature_map is None or len(feature_map.shape) != 4:
        print(f"   ‚ùå –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ü–æ–ª—É—á–∏–ª–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π feature map shape: {feature_map.shape if feature_map is not None else None}")
        print(f"   üîç Target layer was: {target_layer}")
        print("   üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥")
        return extract_features_from_masks_slow(image, masks, model, layer, transform)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä feature map –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ masked pooling
    batch_size, channels, feat_h, feat_w = feature_map.shape
    print(f"   üß™ Feature map @{layer if isinstance(layer, str) else 'layer'}: [{feat_h}, {feat_w}]")  # –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
    
    if feat_h < 18 or feat_w < 18:  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è SEARCHDET_FEAT_SHORT_SIDE=384
        print(f"   ‚ö†Ô∏è Feature map —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π: {feat_h}√ó{feat_w} < 18√ó18")
        print("   üí° –°–æ–≤–µ—Ç—ã: –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å --layer layer2 –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ SEARCHDET_FEAT_SHORT_SIDE=384/512")
        print("   üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞")
        return extract_features_from_masks_slow(image, masks, model, layer, transform)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞—Å–æ–∫ –ø–æ–¥ —Ä–∞–∑–º–µ—Ä feature map
    original_h, original_w = image.shape[:2]
    scale_h = feat_h / original_h
    scale_w = feat_w / original_w
    
    print(f"   üìê Feature map: {channels}√ó{feat_h}√ó{feat_w}, –º–∞—Å—à—Ç–∞–±: {scale_h:.3f}√ó{scale_w:.3f}")
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –º–∞—Å–∫–∏ –≤ –æ–¥–∏–Ω —Ç–µ–Ω–∑–æ—Ä
    mask_tensors = []
    valid_mask_indices = []
    
    for i, mask in enumerate(masks):
        segmentation = mask['segmentation']
        
        # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –º–∞—Å–∫–∏ –ø–æ–¥ feature map (–±–æ–ª–µ–µ –º—è–≥–∫–∏–π —Å–ø–æ—Å–æ–±)
        mask_resized = cv2.resize(segmentation.astype(np.float32), 
                                (feat_w, feat_h), 
                                interpolation=cv2.INTER_LINEAR)
        
        # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö feature map
        if feat_h < 20 or feat_w < 20:
            threshold = 0.01  # –û—á–µ–Ω—å –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö feature map
        else:
            threshold = 0.05  # –£–º–µ–Ω—å—à–∏–ª–∏ —Å 0.1 –¥–æ 0.05
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ—Å–ª–µ resize
        mask_resized = (mask_resized > threshold).astype(bool)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–∞—Å–∫–∞ –Ω–µ –ø—É—Å—Ç–∞—è (–±–æ–ª–µ–µ –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥)
        mask_area = np.sum(mask_resized)
        if mask_area > 0:
            mask_tensors.append(torch.from_numpy(mask_resized).float())
            valid_mask_indices.append(i)
            if i < 5:  # –û—Ç–ª–∞–¥–∫–∞ –¥–ª—è –ø–µ—Ä–≤—ã—Ö –º–∞—Å–æ–∫
                print(f"   üîç –ú–∞—Å–∫–∞ {i}: –∏—Å—Ö–æ–¥–Ω–∞—è {np.sum(segmentation)}, –ø–æ—Å–ª–µ resize {mask_area}")
        else:
            # –ù–µ —Å–æ–∑–¥–∞–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—É—é ¬´–º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –º–∞—Å–∫—É¬ª ‚Äî –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if i < 5:
                print(f"   ‚ö†Ô∏è –ú–∞—Å–∫–∞ {i}: –∏—Å—Ö–æ–¥–Ω–∞—è {np.sum(segmentation)}, –ø–æ—Å–ª–µ resize {mask_area} ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
    
    if len(mask_tensors) == 0:
        print(f"   ‚ùå –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –í—Å–µ {len(masks)} –º–∞—Å–æ–∫ –æ–∫–∞–∑–∞–ª–∏—Å—å –ø—É—Å—Ç—ã–º–∏ –ø–æ—Å–ª–µ —Ä–µ—Å–∞–π–∑–∞!")
        print(f"   üìê –ú–∞—Å—à—Ç–∞–±: {scale_h:.4f}√ó{scale_w:.4f}, feature map: {feat_h}√ó{feat_w}")
        return np.array([])
    
    # –°—Ç–µ–∫–∞–µ–º –º–∞—Å–∫–∏ –≤ –±–∞—Ç—á [M, H', W']
    masks_batch = torch.stack(mask_tensors).to(device)
    
    # üî• –í–ï–ö–¢–û–†–ò–ó–û–í–ê–ù–ù–´–ô Masked Pooling
    # feature_map: [1, C, H', W'] -> [C, H'*W']
    feat_flat = feature_map.squeeze(0).view(channels, -1)
    # masks_batch: [M, H', W'] -> [M, H'*W']
    masks_flat = masks_batch.view(len(mask_tensors), -1)
    
    # –°—É–º–º–∏—Ä—É–µ–º —Ñ–∏—á–∏ –ø–æ –∫–∞–∂–¥–æ–π –º–∞—Å–∫–µ: [M, C]
    masked_sums = torch.mm(masks_flat, feat_flat.t())
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞ –ø–ª–æ—â–∞–¥—å –º–∞—Å–∫–∏
    mask_areas = masks_flat.sum(dim=1, keepdim=True)
    embeddings = masked_sums / (mask_areas + 1e-6)
    
    # Global Average Pooling
    embeddings = F.adaptive_avg_pool1d(embeddings.unsqueeze(-1), 1).squeeze(-1)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN/Inf –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    embeddings = torch.nan_to_num(embeddings, nan=0.0, posinf=1.0, neginf=-1.0)
    
    # L2 –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    embeddings = F.normalize(embeddings, p=2, dim=1)
    
    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ numpy
    result_embeddings = embeddings.cpu().numpy()
    
    print(f"   üîç Embeddings shape: {result_embeddings.shape}, range: [{result_embeddings.min():.6f}, {result_embeddings.max():.6f}]")
    
    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ä—è–¥–æ–∫ (–¥–ª—è –º–∞—Å–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –ø—Ä–æ–ø—É—â–µ–Ω—ã)
    final_embeddings = []
    valid_idx = 0
    embedding_dim = result_embeddings.shape[1] if len(result_embeddings.shape) > 1 and result_embeddings.shape[1] > 0 else 512
    
    for i in range(len(masks)):
        if i in valid_mask_indices:
            if valid_idx < len(result_embeddings):
                emb = result_embeddings[valid_idx]
                # –ï—Å–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π, –¥–µ–ª–∞–µ–º –µ–≥–æ –ø–ª–æ—Å–∫–∏–º
                if len(emb.shape) > 1:
                    emb = emb.flatten()
                final_embeddings.append(emb)
            else:
                final_embeddings.append(np.zeros(embedding_dim))
            valid_idx += 1
        else:
            # –î–ª—è –ø—É—Å—Ç—ã—Ö –º–∞—Å–æ–∫ ‚Äî –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä (–º–∞—Å–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∞)
            final_embeddings.append(np.zeros(embedding_dim))
    
    extract_time = time.time() - extract_start
    old_time_estimate = len(masks) * 0.1  # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è —Å—Ç–∞—Ä–æ–≥–æ –º–µ—Ç–æ–¥–∞
    speedup = old_time_estimate / extract_time if extract_time > 0 else 1
    print(f"   ‚ö° –ë–´–°–¢–†–û: {extract_time:.3f} —Å–µ–∫ ({extract_time/len(masks)*1000:.1f} –º—Å/–º–∞—Å–∫–∞) - —É—Å–∫–æ—Ä–µ–Ω–∏–µ ~{speedup:.1f}x")
    
    return np.array(final_embeddings)


# Function to extract features from masks with OPTIMIZATION (—Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥)
# –£–¥–∞–ª–µ–Ω–æ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Ä–µ–∫—É—Ä—Å–∏–∏


# Function to extract features from masks with OPTIMIZATION
def extract_features_from_masks(image, masks, model, layer, transform):
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–∏—á - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±—ã—Å—Ç—Ä—ã–π Masked Pooling –º–µ—Ç–æ–¥
    """
    # üöÄ –ü—Ä–æ–±—É–µ–º –±—ã—Å—Ç—Ä—ã–π –º–µ—Ç–æ–¥ —Å–Ω–∞—á–∞–ª–∞
    try:
        return extract_features_from_masks_fast(image, masks, model, layer, transform)
    except Exception as e:
        print(f"   ‚ö†Ô∏è –ë—ã—Å—Ç—Ä—ã–π –º–µ—Ç–æ–¥ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª ({str(e)}), –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π")
        return extract_features_from_masks_slow(image, masks, model, layer, transform)


# –°—Ç–∞—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è fallback
def extract_features_from_masks_slow(image, masks, model, layer, transform):
    extract_start = time.time()
    print(f"üß† –ú–ï–î–õ–ï–ù–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(masks)} –º–∞—Å–æ–∫...")
    
    if len(masks) == 0:
        print("   ‚ùå –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ü–µ—Ä–µ–¥–∞–Ω–æ 0 –º–∞—Å–æ–∫ –≤ –º–µ–¥–ª–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é!")
        return np.array([])
    
    # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è DINO –æ–±—Ä–∞–±–æ—Ç–∫–∏
    original_height, original_width = image.shape[:2]
    dino_max_size = 512  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è DINO (–±—ã—Å—Ç—Ä–µ–µ –æ–±—Ä–∞–±–æ—Ç–∫–∞)
    
    if max(original_height, original_width) > dino_max_size:
        dino_scale = dino_max_size / max(original_height, original_width)
        dino_height = int(original_height * dino_scale)
        dino_width = int(original_width * dino_scale)
        use_dino_resize = True
        print(f"   üîß DINO –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: —É–º–µ–Ω—å—à–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–æ {dino_width}x{dino_height} –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è")
    else:
        use_dino_resize = False
        print(f"   ‚úÖ DINO: –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä {original_width}x{original_height}")
    
    features = []
    for i, mask in enumerate(masks):
        if i % 50 == 0 and i > 0:  # –ü—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 50 –º–∞—Å–æ–∫
            elapsed = time.time() - extract_start
            estimated_total = elapsed * len(masks) / i
            print(f"   üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(masks)} –º–∞—Å–æ–∫ ({elapsed:.1f}—Å, –æ—Å—Ç–∞–ª–æ—Å—å ~{estimated_total-elapsed:.1f}—Å)")
        
        segmentation = mask['segmentation']
        mask_image = np.zeros_like(image)
        mask_image[segmentation] = image[segmentation]
        
        # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –£–º–µ–Ω—å—à–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è DINO –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if use_dino_resize:
            # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –∫–∞–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Ç–∞–∫ –∏ –º–∞—Å–∫–∏
            resized_image = cv2.resize(mask_image, (dino_width, dino_height), interpolation=cv2.INTER_LINEAR)
            pil_image = Image.fromarray(resized_image)
        else:
            pil_image = Image.fromarray(mask_image)
            
        features.append(get_vector(pil_image, model, layer, transform).cpu().numpy())
    
    extract_time = time.time() - extract_start
    print(f"   ‚è±Ô∏è –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞: {extract_time:.3f} —Å–µ–∫ ({extract_time/len(masks)*1000:.1f} –º—Å/–º–∞—Å–∫–∞)")
    print(f"   ‚úÖ –°—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(features)} –∏–∑ {len(masks)} –º–∞—Å–æ–∫")
    
    return np.array(features)


# Function to calculate softmax attention weights
def calculate_attention_weights_softmax(query_embedding, example_embeddings):
    similarities = cosine_similarity(query_embedding.reshape(1, -1), example_embeddings).flatten()
    exp_similarities = np.exp(similarities)
    attention_weights = exp_similarities / np.sum(exp_similarities)
    return attention_weights


# Function to adjust the query embedding with STRONGER emphasis on positive examples
def adjust_embedding(query_embedding, positive_embeddings, negative_embeddings, positive_weight=3.0, negative_weight=1.5):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å —Å–∏–ª—å–Ω—ã–º —É–ø–æ—Ä–æ–º –Ω–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
    –∏ —É—Å–∏–ª–µ–Ω–Ω—ã–º –æ—Ç—Ç–∞–ª–∫–∏–≤–∞–Ω–∏–µ–º –æ—Ç —Ñ–æ–Ω–æ–≤—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π
    
    Args:
        positive_weight: –í–µ—Å –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (—É–≤–µ–ª–∏—á–µ–Ω –¥–æ 3.0)
        negative_weight: –í–µ—Å –¥–ª—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (—É–≤–µ–ª–∏—á–µ–Ω –¥–æ 1.5 –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ç—Ç–∞–ª–∫–∏–≤–∞–Ω–∏—è)
    """
    positive_weights = calculate_attention_weights_softmax(query_embedding, positive_embeddings)
    negative_weights = calculate_attention_weights_softmax(query_embedding, negative_embeddings)

    # –£–°–ò–õ–ï–ù–ù–´–ô —É–ø–æ—Ä –Ω–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
    positive_adjustment = positive_weight * np.sum(positive_weights[:, np.newaxis] * positive_embeddings, axis=0)
    
    # –£–°–ò–õ–ï–ù–ù–û–ï –æ—Ç—Ç–∞–ª–∫–∏–≤–∞–Ω–∏–µ –æ—Ç —Ñ–æ–Ω–æ–≤—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π
    negative_adjustment = negative_weight * np.sum(negative_weights[:, np.newaxis] * negative_embeddings, axis=0)

    # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –≤–∫–ª—é—á–∏–≤ –∏—Å—Ö–æ–¥–Ω—ã–π query, —É—Å–∏–ª–∏–≤–∞–µ–º –ø–æ–∑–∏—Ç–∏–≤ –∏ –æ—Ç—Ç–∞–ª–∫–∏–≤–∞–µ–º –Ω–µ–≥–∞—Ç–∏–≤
    combined_adjustment = query_embedding + positive_adjustment - negative_adjustment
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    norm = np.linalg.norm(combined_adjustment)
    if norm > 0:
        combined_adjustment = combined_adjustment / norm
    else:
        # Fallback –µ—Å–ª–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å
        combined_adjustment = query_embedding
        
    return combined_adjustment


# Function to annotate image with bounding boxes
def annotate_image(
    example_img, 
    query_vectors, 
    resnet_model, 
    layer, 
    transform, 
    sam, 
    output_image_path,
    iou_threshold: float = 0.5,
    # conf_threshold: float = 0.3,
):
    total_annotation_start = time.time()
    print("üéØ –ù–∞—á–∞–ª–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
    
    # Generate masks using SAM
    sam_gen_start = time.time()
    print("üî™ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∞—Å–æ–∫ —Å –ø–æ–º–æ—â—å—é SAM-HQ...")
    mask_generator = SamAutomaticMaskGenerator(
        model=sam,
        points_per_side=32,
        points_per_batch=64,
        pred_iou_thresh=0.8,
        stability_score_thresh=0.9,
        crop_n_layers=1,
        crop_n_points_downscale_factor=2,
        min_mask_region_area=100,
        box_nms_thresh=iou_threshold,
        crop_nms_thresh=iou_threshold,
        crop_overlap_ratio=512 / 1500,
    )

    masks = mask_generator.generate(np.array(example_img))
    sam_gen_time = time.time() - sam_gen_start
    print(f"   ‚è±Ô∏è SAM-HQ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª {len(masks)} –º–∞—Å–æ–∫ –∑–∞: {sam_gen_time:.3f} —Å–µ–∫")

    # Extract mask vectors
    example_img_np = np.array(example_img)
    mask_vectors = extract_features_from_masks(example_img_np, masks, resnet_model, layer, transform)
    mask_vectors = np.array(mask_vectors, dtype=np.float32)

    # Normalize query and mask vectors
    norm_start = time.time()
    print("üìê –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤...")
    query_vectors = query_vectors / np.linalg.norm(query_vectors, axis=1, keepdims=True)
    mask_vectors = mask_vectors / np.linalg.norm(mask_vectors, axis=1, keepdims=True)
    norm_time = time.time() - norm_start
    print(f"   ‚è±Ô∏è –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞: {norm_time:.3f} —Å–µ–∫")

    # Create FAISS index and add query vectors
    faiss_start = time.time()
    print("üîç FAISS –ø–æ–∏—Å–∫ —Å—Ö–æ–¥—Å—Ç–≤–∞...")
    index = faiss.IndexFlatIP(mask_vectors.shape[1])  # dim –ø–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Ñ–∏—á
    index.add(query_vectors)

    # Search for matches in the FAISS index
    similarities, indices = index.search(mask_vectors, 1)
    faiss_time = time.time() - faiss_start
    print(f"   ‚è±Ô∏è FAISS –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞: {faiss_time:.3f} —Å–µ–∫")

    # Map similarities to [0, 1]
    filter_start = time.time()
    print("üéØ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ threshold...")
    normalized_similarities = (similarities + 1) / 2

    # Apply a threshold to filter matches
    threshold = 0.474
    filtered_indices = np.where(normalized_similarities > threshold)[0]
    filter_time = time.time() - filter_start
    print(f"   ‚è±Ô∏è –ù–∞–π–¥–µ–Ω–æ {len(filtered_indices)} –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –º–∞—Å–æ–∫ –∑–∞: {filter_time:.3f} —Å–µ–∫")

    # Draw bounding boxes for detected objects
    draw_start = time.time()
    print("üé® –†–∏—Å–æ–≤–∞–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π...")
    example_img_cv = cv2.cvtColor(np.array(example_img), cv2.COLOR_RGB2BGR)
    for idx in filtered_indices:
        mask = masks[idx]
        segmentation = mask['segmentation']
        coords = np.column_stack(np.where(segmentation))
        y_min, x_min = coords.min(axis=0)
        y_max, x_max = coords.max(axis=0)
        cv2.rectangle(example_img_cv, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)

    # Convert back to RGB and save the annotated image
    example_img_cv = cv2.cvtColor(example_img_cv, cv2.COLOR_BGR2RGB)
    annotated_img = Image.fromarray(example_img_cv)
    annotated_img.save(output_image_path)
    draw_time = time.time() - draw_start
    print(f"   ‚è±Ô∏è –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –Ω–∞—Ä–∏—Å–æ–≤–∞–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∑–∞: {draw_time:.3f} —Å–µ–∫")
    
    total_annotation_time = time.time() - total_annotation_start
    print(f"üéâ –û–±—â–µ–µ –≤—Ä–µ–º—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: {total_annotation_time:.3f} —Å–µ–∫")
    print(f"üìä –†–∞–∑–±–∏–≤–∫–∞ –≤—Ä–µ–º–µ–Ω–∏:")
    print(f"   üî™ SAM –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∞—Å–æ–∫: {sam_gen_time:.3f}—Å ({sam_gen_time/total_annotation_time*100:.1f}%)")
    print(f"   üß† –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {extract_time:.3f}—Å ({extract_time/total_annotation_time*100:.1f}%)")
    print(f"   üìê –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: {norm_time:.3f}—Å ({norm_time/total_annotation_time*100:.1f}%)")
    print(f"   üîç FAISS –ø–æ–∏—Å–∫: {faiss_time:.3f}—Å ({faiss_time/total_annotation_time*100:.1f}%)")
    print(f"   üéØ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: {filter_time:.3f}—Å ({filter_time/total_annotation_time*100:.1f}%)")
    print(f"   üé® –†–∏—Å–æ–≤–∞–Ω–∏–µ: {draw_time:.3f}—Å ({draw_time/total_annotation_time*100:.1f}%)")