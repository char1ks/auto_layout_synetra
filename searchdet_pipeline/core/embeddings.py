#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –º–∞—Å–æ–∫ –∏ –ø—Ä–∏–º–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ SearchDet.
"""

import numpy as np
import cv2
from PIL import Image
import torch
import torchvision.transforms as transforms
try:
    from mask_withsearch import get_vector, adjust_embedding, extract_features_from_masks
    SEARCHDET_AVAILABLE = True
except Exception as e:
    print(f"‚ö†Ô∏è SearchDet –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    SEARCHDET_AVAILABLE = False

class EmbeddingExtractor:
    def __init__(self, detector):
        self.detector = detector
        self.backbone = getattr(detector, 'backbone', 'dinov2_b')
        # –õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DINO –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
        self._dino_model = None
        self._dino_preprocess = None
        # üöÄ –ö—ç—à –¥–ª—è DINO forward pass
        self._dino_cache = {}  # {image_hash: (patch_tokens, grid_size)}
        # üöÄ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.max_embedding_size = getattr(detector, 'max_embedding_size', 1024)
        # üöÄ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–ª–æ–≤–∏–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–ª—è DINO
        self.dino_half_precision = getattr(detector, 'dino_half_precision', False)
    
    def extract_mask_embeddings(self, image_np, masks):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –º–∞—Å–æ–∫."""
        print("üß† –≠–¢–ê–ü 2: –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –º–∞—Å–æ–∫ –∏ –∑–∞–ø—Ä–æ—Å–æ–≤...")
        print(f"üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(masks)} –º–∞—Å–æ–∫...")
        
        if not SEARCHDET_AVAILABLE:
            print("‚ö†Ô∏è SearchDet –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏")
            return np.zeros((0, 1024), dtype=np.float32), []
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ PIL
        pil_image = Image.fromarray(image_np)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–∞—Å–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ boolean numpy array —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
        mask_arrays = []
        valid_indices = []
        min_mask_area = 100  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–ª–æ—â–∞–¥—å –º–∞—Å–∫–∏ –≤ –ø–∏–∫—Å–µ–ª—è—Ö
        
        for i, mask_dict in enumerate(masks):
            mask = mask_dict["segmentation"]
            if isinstance(mask, np.ndarray) and mask.dtype == bool:
                # üöÄ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–∞–∑–º–µ—Ä—É —Å—Ä–∞–∑—É –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ
                mask_area = np.sum(mask)
                if mask_area >= min_mask_area:
                    mask_arrays.append(mask)
                    valid_indices.append(i)
            else:
                print(f"   ‚ö†Ô∏è –ú–∞—Å–∫–∞ {i} –∏–º–µ–µ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ç–∏–ø: {type(mask)}")
        
        if len(mask_arrays) < len(masks):
            print(f"   üîç –ü—Ä–µ–¥—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –º–∞—Å–æ–∫: {len(masks)} ‚Üí {len(mask_arrays)} (—É–¥–∞–ª–µ–Ω–æ {len(masks) - len(mask_arrays)} –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö/–º–∞–ª–µ–Ω—å–∫–∏—Ö –º–∞—Å–æ–∫)")
        
        if not mask_arrays:
            print("   ‚ùå –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –º–∞—Å–æ–∫ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
            return np.zeros((0, 1024), dtype=np.float32), []
        
        print(f"üöÄ –ë–´–°–¢–†–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(mask_arrays)} –º–∞—Å–æ–∫ (Masked Pooling)...")
        
        try:
            embeddings = self._extract_fast(image_np, mask_arrays)
            if embeddings is not None:
                print(f"‚ö° –ë–´–°–¢–†–û: {len(mask_arrays)} –º–∞—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ")
                # --- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–æ—Ä–º –≤–æ–∑–≤—Ä–∞—Ç–∞ –∫ (N, D) float32 ---
                embeddings = np.asarray(embeddings)
                if embeddings.ndim == 1:
                    embeddings = embeddings.reshape(1, -1)
                elif embeddings.ndim > 2:
                    embeddings = embeddings.reshape(embeddings.shape[0], -1)
                embeddings = embeddings.astype(np.float32)
                return embeddings, valid_indices
        except Exception as e:
            print(f"‚ö†Ô∏è –ë—ã—Å—Ç—Ä—ã–π –º–µ—Ç–æ–¥ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª ({e}), –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π")
        
        print(f"üß† –ú–ï–î–õ–ï–ù–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(mask_arrays)} –º–∞—Å–æ–∫...")
        try:
            embeddings = self._extract_slow(pil_image, mask_arrays)
            if embeddings is not None:
                print(f"‚úÖ –°—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(mask_arrays)} –º–∞—Å–æ–∫")
                embeddings = np.asarray(embeddings)
                if embeddings.ndim == 1:
                    embeddings = embeddings.reshape(1, -1)
                elif embeddings.ndim > 2:
                    embeddings = embeddings.reshape(embeddings.shape[0], -1)
                embeddings = embeddings.astype(np.float32)
                return embeddings, valid_indices
        except Exception as e:
            print(f"‚ö†Ô∏è –ú–µ–¥–ª–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
        
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏")
        return np.zeros((0, 1024), dtype=np.float32), []
    
    def _extract_fast(self, image_np, mask_arrays):
        import time
        
        if self.backbone.startswith('dinov2'):
            return self._extract_with_dino(image_np, mask_arrays)
            
        extract_start = time.time()
        print(f"üöÄ –ë–´–°–¢–†–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(mask_arrays)} –º–∞—Å–æ–∫ (Masked Pooling)...")
        
        resnet, layer, transform, sam = (
            self.detector.searchdet_resnet,
            self.detector.searchdet_layer, 
            self.detector.searchdet_transform,
            self.detector.searchdet_sam
        )
        
        print(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º {layer} –¥–ª—è –±–æ–ª—å—à–µ–≥–æ feature map")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ tuple –∏–∑ –º–æ–¥–µ–ª–µ–π
        if isinstance(resnet, tuple) and len(resnet) >= 2:
            model = resnet[0]  # backbone
            device = resnet[1] if len(resnet) > 1 else 'cuda'
        else:
            model = resnet
            device = 'cuda'
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º mask_arrays –≤ —Ñ–æ—Ä–º–∞—Ç, –æ–∂–∏–¥–∞–µ–º—ã–π extract_features_from_masks
        # –§—É–Ω–∫—Ü–∏—è –æ–∂–∏–¥–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–æ–º 'segmentation'
        mask_dicts = []
        for mask_array in mask_arrays:
            mask_dicts.append({'segmentation': mask_array})
        
        # üöÄ –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±—ã—Å—Ç—Ä—ã–π –º–µ—Ç–æ–¥ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            if model is not None and layer is not None:
                embeddings = extract_features_from_masks(image_np, mask_dicts, model, layer, transform)
                if isinstance(embeddings, np.ndarray) and embeddings.ndim == 2 and embeddings.shape[0] == len(mask_arrays):
                    extract_time = time.time() - extract_start
                    old_time_estimate = len(mask_arrays) * 0.1  # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è —Å—Ç–∞—Ä–æ–≥–æ –º–µ—Ç–æ–¥–∞
                    speedup = old_time_estimate / extract_time if extract_time > 0 else 1
                    print(f"   ‚ö° –ë–´–°–¢–†–û: {extract_time:.3f} —Å–µ–∫ ({extract_time/len(mask_arrays)*1000:.1f} –º—Å/–º–∞—Å–∫–∞) - —É—Å–∫–æ—Ä–µ–Ω–∏–µ ~{speedup:.1f}x")
                    return embeddings
        except Exception as e:
            print(f"   ‚ö†Ô∏è –ë—ã—Å—Ç—Ä—ã–π –º–µ—Ç–æ–¥ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
        
        # Fallback: –ø—Ä–æ–±—É–µ–º –±–µ–∑ transform
        try:
            if model is not None and layer is not None:
                embeddings = extract_features_from_masks(image_np, mask_dicts, model, layer, None)
                if isinstance(embeddings, np.ndarray) and embeddings.ndim == 2:
                    extract_time = time.time() - extract_start
                    print(f"   ‚ö° –ë–´–°–¢–†–û (–±–µ–∑ transform): {extract_time:.3f} —Å–µ–∫")
                    return embeddings
        except Exception as e:
            print(f"   ‚ö†Ô∏è –ë—ã—Å—Ç—Ä—ã–π –º–µ—Ç–æ–¥ –±–µ–∑ transform –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
        
        return None
    
    def _extract_slow(self, pil_image, mask_arrays):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –º–µ–¥–ª–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."""
        import time
        import cv2
        
        extract_start = time.time()
        print(f"üêå –ú–ï–î–õ–ï–ù–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(mask_arrays)} –º–∞—Å–æ–∫...")
        
        if self.backbone.startswith('dinov2'):
            return self._extract_with_dino(np.array(pil_image), mask_arrays)
        
        # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ SearchDet
        resnet, layer, transform, sam = (
            self.detector.searchdet_resnet,
            self.detector.searchdet_layer, 
            self.detector.searchdet_transform,
            self.detector.searchdet_sam
        )
        
        print(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º {layer} –¥–ª—è –±–æ–ª—å—à–µ–≥–æ feature map")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ tuple –∏–∑ –º–æ–¥–µ–ª–µ–π
        if isinstance(resnet, tuple) and len(resnet) >= 2:
            model = resnet[0]  # backbone
            device = resnet[1] if len(resnet) > 1 else 'cuda'
        else:
            model = resnet
            device = 'cuda'
        
        # üöÄ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        original_image = np.array(pil_image)
        h, w = original_image.shape[:2]
        
        if max(h, w) > self.max_embedding_size:
            scale = self.max_embedding_size / max(h, w)
            new_h, new_w = int(h * scale), int(w * scale)
            scaled_image = cv2.resize(original_image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
            print(f"   üìè –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {w}x{h} ‚Üí {new_w}x{new_h} (scale={scale:.3f}, max_size={self.max_embedding_size})")
            
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –º–∞—Å–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ
            scaled_mask_arrays = []
            for mask_array in mask_arrays:
                scaled_mask = cv2.resize(mask_array.astype(np.uint8), (new_w, new_h), interpolation=cv2.INTER_NEAREST)
                scaled_mask_arrays.append(scaled_mask.astype(bool))
            mask_arrays = scaled_mask_arrays
            pil_image = Image.fromarray(scaled_image)
        
        # –ü—Ä–æ–±—É–µ–º –±–∞—Ç—á–µ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —á–µ—Ä–µ–∑ extract_features_from_masks_slow
        try:
            mask_dicts = []
            for mask_array in mask_arrays:
                mask_dicts.append({'segmentation': mask_array})
            
            image_np = np.array(pil_image)
            
            from mask_withsearch import extract_features_from_masks_slow
            embeddings = extract_features_from_masks_slow(image_np, mask_dicts, model, layer, transform)
            
            if isinstance(embeddings, np.ndarray) and embeddings.ndim == 2:
                extract_time = time.time() - extract_start
                print(f"   üêå –ú–ï–î–õ–ï–ù–ù–û (batch): {extract_time:.3f} —Å–µ–∫ ({extract_time/len(mask_arrays)*1000:.1f} –º—Å/–º–∞—Å–∫–∞)")
                return embeddings
                
        except Exception as e:
            print(f"   ‚ö†Ô∏è –ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞: {e}")
        
        # Fallback: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –º–∞—Å–∫–∏ –ø–æ –æ–¥–Ω–æ–π
        print(f"   üîÑ Fallback: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –º–∞—Å–∫–∏ –ø–æ –æ–¥–Ω–æ–π...")
        embeddings = []
        for i, mask in enumerate(mask_arrays):
            try:
                image_np = np.array(pil_image)
                mask_image = np.zeros_like(image_np)
                mask_image[mask] = image_np[mask]
                
                mask_pil = Image.fromarray(mask_image)
                
                vec = get_vector(mask_pil, model, layer, transform)
                if hasattr(vec, 'numpy'):
                    embeddings.append(vec.numpy())
                else:
                    embeddings.append(vec)
                    
            except Exception as e:
                print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å –º–∞—Å–∫–æ–π {i}: {e}")
                embeddings.append(np.random.rand(1024).astype(np.float32))
        
        if embeddings:
            embeddings_array = np.array(embeddings)
            norms = np.linalg.norm(embeddings_array, axis=1, keepdims=True)
            embeddings_array = embeddings_array / (norms + 1e-8)
            extract_time = time.time() - extract_start
            print(f"   üêå –ú–ï–î–õ–ï–ù–ù–û (–ø–æ –æ–¥–Ω–æ–π): {extract_time:.3f} —Å–µ–∫ ({extract_time/len(mask_arrays)*1000:.1f} –º—Å/–º–∞—Å–∫–∞)")
            return embeddings_array
        
        return None
    
    def build_queries(self, pos_imgs, neg_imgs):
        """–°—Ç—Ä–æ–∏—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ positive/negative –ø—Ä–∏–º–µ—Ä–æ–≤."""
        print(f"üîç –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ {len(pos_imgs)} positive –∏ {len(neg_imgs)} negative –ø—Ä–∏–º–µ—Ä–æ–≤")
        if self.backbone.startswith('dinov2'):
            # DINO –ø—É—Ç—å: –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –±–µ–∑ –º–∞—Å–æ–∫
            pos_list = []
            for i, img in enumerate(pos_imgs):
                try:
                    vec = self._get_dino_global(np.array(img))
                    if vec is not None:
                        pos_list.append(vec)
                except Exception as e:
                    print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å positive {i}: {e}")
            neg_list = []
            for i, img in enumerate(neg_imgs):
                try:
                    vec = self._get_dino_global(np.array(img))
                    if vec is not None:
                        neg_list.append(vec)
                except Exception as e:
                    print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å negative {i}: {e}")
            q_pos = np.array(pos_list) if pos_list else np.array([]).reshape(0, 1024)
            q_neg = np.array(neg_list) if neg_list else np.array([]).reshape(0, 1024)
            # L2 –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            if q_pos.shape[0] > 0:
                q_pos = q_pos / (np.linalg.norm(q_pos, axis=1, keepdims=True) + 1e-8)
            if q_neg.shape[0] > 0:
                q_neg = q_neg / (np.linalg.norm(q_neg, axis=1, keepdims=True) + 1e-8)
            print(f"   üìä –ü–æ—Å—Ç—Ä–æ–µ–Ω–æ positive: {q_pos.shape[0]}, negative: {q_neg.shape[0]}")
            return q_pos.astype(np.float32), q_neg.astype(np.float32)

        # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ SearchDet
        resnet, layer, transform, sam = (
            self.detector.searchdet_resnet,
            self.detector.searchdet_layer, 
            self.detector.searchdet_transform,
            self.detector.searchdet_sam
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ tuple –∏–∑ –º–æ–¥–µ–ª–µ–π
        if isinstance(resnet, tuple) and len(resnet) >= 2:
            model = resnet[0]  # backbone
            device = resnet[1] if len(resnet) > 1 else 'cuda'
        else:
            model = resnet
            device = 'cuda'
        
        # Positive —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        pos_list = []
        for i, img in enumerate(pos_imgs):
            try:
                vec = self._get_image_embedding(img, model, layer, transform)
                if vec is not None:
                    pos_list.append(vec)
            except Exception as e:
                print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å positive {i}: {e}")
        
        # Negative —ç–º–±–µ–¥–¥–∏–Ω–≥–∏  
        neg_list = []
        for i, img in enumerate(neg_imgs):
            try:
                vec = self._get_image_embedding(img, model, layer, transform)
                if vec is not None:
                    neg_list.append(vec)
            except Exception as e:
                print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å negative {i}: {e}")
        
        q_pos = np.array(pos_list) if pos_list else np.array([]).reshape(0, 1024)
        q_neg = np.array(neg_list) if neg_list else np.array([]).reshape(0, 1024)
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ positive —Å —É—á—ë—Ç–æ–º negative (–∫–∞–∫ –≤ hybrid)
        if q_pos.shape[0] > 0 and q_neg.shape[0] > 0:
            try:
                adjusted = np.stack([adjust_embedding(q, q_pos, q_neg) for q in q_pos], axis=0).astype(np.float32)
                q_pos = adjusted
            except Exception as e:
                print(f"   ‚ö†Ô∏è adjust_embedding error, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ: {e}")
        
        # L2-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        if q_pos.shape[0] > 0:
            q_pos = q_pos / (np.linalg.norm(q_pos, axis=1, keepdims=True) + 1e-8)
        if q_neg.shape[0] > 0:
            q_neg = q_neg / (np.linalg.norm(q_neg, axis=1, keepdims=True) + 1e-8)
        
        print(f"   üìä –ü–æ—Å—Ç—Ä–æ–µ–Ω–æ positive: {q_pos.shape[0]}, negative: {q_neg.shape[0]}")
        return q_pos.astype(np.float32), q_neg.astype(np.float32)
    
    def _get_image_embedding(self, pil_image, model, layer, transform):
        """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        if not SEARCHDET_AVAILABLE:
            # –ó–∞–≥–ª—É—à–∫–∞
            return np.random.rand(1024).astype(np.float32)
        
        if self.backbone.startswith('dinov2'):
            return self._get_dino_global(np.array(pil_image))

        try:
            vec = get_vector(pil_image, model, layer, transform)
            if isinstance(vec, np.ndarray):
                return vec
            
            # –ï—Å–ª–∏ —ç—Ç–æ —Ç–µ–Ω–∑–æ—Ä PyTorch
            if hasattr(vec, 'numpy'):
                return vec.numpy()
                
            return None
        except Exception as e:
            print(f"   ‚ö†Ô∏è get_vector failed: {e}")
            
            try:
                # Fallback - —Å–ª—É—á–∞–π–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
                return np.random.rand(1024).astype(np.float32)
            except:
                return None

    def _ensure_dino(self):
        if self._dino_model is not None:
            return
        try:
            import timm
            import torchvision.transforms as T
            from torchvision.transforms import InterpolationMode
            
            name_map = {
                'dinov2_s': 'vit_small_patch14_dinov2.lvd142m',
                'dinov2_b': 'vit_base_patch14_dinov2.lvd142m',
                'dinov2_l': 'vit_large_patch14_dinov2.lvd142m',
                'dinov2_g': 'vit_giant_patch14_dinov2.lvd142m',
            }
            model_name = name_map.get(self.backbone, 'vit_base_patch14_dinov2.lvd142m')
            self._dino_model = timm.create_model(model_name, pretrained=True)
            self._dino_model.eval()
            
            # üöÄ –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ–ª–æ–≤–∏–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞
            if self.dino_half_precision:
                self._dino_model = self._dino_model.half()
                print(f"   ‚ö° DINO –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–∞ –≤ float16 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è")
            
            data_config = self._dino_model.default_cfg
            self.dino_img_size = data_config['input_size'][-1]
            
            try:
                patch_size = self._dino_model.patch_embed.patch_size[0]
                self.dino_grid_size = (self.dino_img_size // patch_size, self.dino_img_size // patch_size)
            except Exception:
                self.dino_grid_size = (37, 37)
            class SquarePad:
                def __call__(self, image):
                    w, h = image.size
                    max_wh = np.max([w, h])
                    hp = (max_wh - w) // 2
                    vp = (max_wh - h) // 2
                    padding = (hp, vp, max_wh - w - hp, max_wh - h - vp)
                    return T.functional.pad(image, padding, 0, 'constant')

            self._dino_preprocess = T.Compose([
                SquarePad(),
                T.Resize(self.dino_img_size, interpolation=InterpolationMode.BICUBIC, antialias=True),
                T.CenterCrop(self.dino_img_size),
                T.ToTensor(),
                T.Normalize(mean=data_config['mean'], std=data_config['std']),
            ])
            
            print(f"üß© DINO backbone={self.backbone}, img_size={self.dino_img_size}, grid={self.dino_grid_size}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å DINOv2: {e}")
            self._dino_model = None

    def _get_dino_global(self, image_np: np.ndarray):
        self._ensure_dino()
        if self._dino_model is None:
            return np.random.rand(1024).astype(np.float32)
        import torch
        with torch.no_grad():
            pil = Image.fromarray(image_np)
            x = self._dino_preprocess(pil).unsqueeze(0)
            
            # üöÄ –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ–ª–æ–≤–∏–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if self.dino_half_precision:
                x = x.half()
            
            feats = self._dino_model.forward_features(x)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã—Ö–æ–¥–∞: –º–æ–∂–µ—Ç –±—ã—Ç—å dict –∏–ª–∏ tensor
            if isinstance(feats, dict):
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: CLS —Ç–æ–∫–µ–Ω, –µ—Å–ª–∏ –µ—Å—Ç—å
                if 'x_norm_clstoken' in feats:
                    vec = feats['x_norm_clstoken'][0]
                # Fallback: —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –ø–∞—Ç—á–∞–º
                elif 'x_norm_patchtokens' in feats:
                    vec = feats['x_norm_patchtokens'][0].mean(dim=0)
                else: # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç dict
                    vec = next(iter(feats.values()))[0].mean(dim=0)
            elif torch.is_tensor(feats):
                # –ï—Å–ª–∏ feats - —Ç–µ–Ω–∑–æ—Ä, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º (B, N, D)
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º CLS —Ç–æ–∫–µ–Ω (–ø–µ—Ä–≤—ã–π) –∫–∞–∫ –≥–ª–æ–±–∞–ª—å–Ω—ã–π –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä
                if feats.ndim == 3 and feats.shape[1] > 0:
                    vec = feats[0, 0]
                # Fallback: —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –≤—Å–µ–º —Ç–æ–∫–µ–Ω–∞–º/–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤—É
                else:
                    vec = feats[0].mean(dim=0)
            else: # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø
                return np.random.rand(1024).astype(np.float32)

            vec = vec.detach().cpu().float().numpy().squeeze()
            
            # –ü—Ä–∏–≤–æ–¥–∏–º –∫ 1024 –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            if vec.shape[0] != 1024:
                out = np.zeros(1024, dtype=np.float32)
                take = min(1024, vec.shape[0])
                out[:take] = vec[:take]
                vec = out
                
            # L2 norm
            vec_norm = np.linalg.norm(vec)
            if vec_norm > 1e-8:
                vec = vec / vec_norm
            return vec.astype(np.float32)

    def _extract_with_dino(self, image_np, mask_arrays):
        import time
        import torch
        import torch.nn.functional as F
        import cv2
        
        extract_start = time.time()
        print(f"üöÄ –ë–´–°–¢–†–û–ï DINO –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(mask_arrays)} –º–∞—Å–æ–∫...")
        
        self._ensure_dino()
        if self._dino_model is None:
            return None

        # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        H0, W0 = image_np.shape[:2]
        scale = 1.0
        
        if max(H0, W0) > self.max_embedding_size:
            if H0 >= W0:
                scale = self.max_embedding_size / float(H0)
            else:
                scale = self.max_embedding_size / float(W0)
            scaled_image = cv2.resize(image_np, (int(W0 * scale), int(H0 * scale)), interpolation=cv2.INTER_LINEAR)
            print(f"   üîß –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ DINO: {W0}x{H0} ‚Üí {scaled_image.shape[1]}x{scaled_image.shape[0]} (scale={scale:.3f}, max_size={self.max_embedding_size})")
        else:
            scaled_image = image_np

        # üöÄ –ö–≠–®–ò–†–û–í–ê–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        import hashlib
        image_hash = hashlib.md5(scaled_image.tobytes()).hexdigest()
        
        if image_hash in self._dino_cache:
            patch_tokens, cached_grid_size = self._dino_cache[image_hash]
            print(f"   ‚ö° DINO –∫—ç—à –ø–æ–ø–∞–¥–∞–Ω–∏–µ! –ü—Ä–æ–ø—É—Å–∫–∞–µ–º forward pass")
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å grid —Ä–∞–∑–º–µ—Ä–∞
            if cached_grid_size != self.dino_grid_size:
                print(f"   ‚ö†Ô∏è Grid —Ä–∞–∑–º–µ—Ä –∏–∑–º–µ–Ω–∏–ª—Å—è: {cached_grid_size} ‚Üí {self.dino_grid_size}, –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º")
                del self._dino_cache[image_hash]
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
                pass
        
        if image_hash not in self._dino_cache:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ DINO
            pil_image = Image.fromarray(scaled_image)
            x = self._dino_preprocess(pil_image).unsqueeze(0)
            
            # üöÄ –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ–ª–æ–≤–∏–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if self.dino_half_precision:
                x = x.half()
            
            dino_start = time.time()
            with torch.no_grad():
                feats = self._dino_model.forward_features(x)
            dino_time = time.time() - dino_start
            print(f"   ‚ö° DINO forward: {dino_time:.3f}—Å (precision: {'float16' if self.dino_half_precision else 'float32'})")
            
            # --- –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ç—á-—Ç–æ–∫–µ–Ω–æ–≤ ---
            patch_tokens = None
            if isinstance(feats, dict) and 'x_norm_patchtokens' in feats:
                patch_tokens = feats['x_norm_patchtokens'][0]
            elif torch.is_tensor(feats) and feats.ndim == 3 and feats.shape[1] > 1:
                # –ï—Å–ª–∏ feats - —Ç–µ–Ω–∑–æ—Ä (B, N, D), –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º CLS —Ç–æ–∫–µ–Ω
                patch_tokens = feats[0, 1:]
            
            if patch_tokens is None:
                print("‚ö†Ô∏è DINO model did not return patch tokens. Falling back to old method.")
                return self._extract_with_dino_fallback(image_np, mask_arrays)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Ä–∞–∑–º–µ—Ä–∞ grid
            expected_tokens = self.dino_grid_size[0] * self.dino_grid_size[1]
            if patch_tokens.shape[0] != expected_tokens:
                print(f"‚ö†Ô∏è Mismatch in token count: expected {expected_tokens}, got {patch_tokens.shape[0]}. Fallback.")
                return self._extract_with_dino_fallback(image_np, mask_arrays)
            
            # üöÄ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            self._dino_cache[image_hash] = (patch_tokens.clone(), self.dino_grid_size)
            print(f"   üíæ DINO —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –∫—ç—à (—Ä–∞–∑–º–µ—Ä –∫—ç—à–∞: {len(self._dino_cache)})")
        else:
             patch_tokens, _ = self._dino_cache[image_hash]

        # üöÄ –°–£–ü–ï–†-–ë–ê–¢–ß–ï–í–ê–Ø –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –º–∞—Å–æ–∫
        embeddings = []
        gh, gw = self.dino_grid_size
        
        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –≤—Å–µ –º–∞—Å–∫–∏ –∫ —Ä–∞–∑–º–µ—Ä—É scaled_image
        scaled_masks = []
        for mask in mask_arrays:
            if scale != 1.0:
                # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –º–∞—Å–∫—É –∫ —Ä–∞–∑–º–µ—Ä—É scaled_image
                scaled_mask = cv2.resize(mask.astype(np.uint8), (scaled_image.shape[1], scaled_image.shape[0]), interpolation=cv2.INTER_NEAREST)
                scaled_mask = scaled_mask.astype(bool)
            else:
                scaled_mask = mask
            scaled_masks.append(scaled_mask)
        
        # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ë–∞—Ç—á–µ–≤–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –≤—Å–µ—Ö –º–∞—Å–æ–∫ —Å—Ä–∞–∑—É
        batch_start = time.time()
        valid_masks = []
        valid_indices = []
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –Ω–µ–ø—É—Å—Ç—ã–µ –º–∞—Å–∫–∏ –≤ –æ–¥–∏–Ω –±–∞—Ç—á
        for i, mask in enumerate(scaled_masks):
            if mask.sum() > 0:
                valid_masks.append(mask.astype(np.float32))
                valid_indices.append(i)
        
        if valid_masks:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –±–∞—Ç—á —Ç–µ–Ω–∑–æ—Ä (N, 1, H, W)
            batch_masks = torch.from_numpy(np.stack(valid_masks)).unsqueeze(1)
            
            # –ë–∞—Ç—á–µ–≤–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –≤—Å–µ—Ö –º–∞—Å–æ–∫ —Å—Ä–∞–∑—É
            resized_batch = F.interpolate(batch_masks, size=(gh, gw), mode='bilinear', align_corners=False)
            resized_batch = resized_batch.squeeze(1).view(len(valid_masks), -1)  # (N, gh*gw)
            
            batch_time = time.time() - batch_start
            print(f"   ‚ö° –ë–∞—Ç—á–µ–≤–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è {len(valid_masks)} –º–∞—Å–æ–∫: {batch_time:.3f}—Å")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –≤–∞–ª–∏–¥–Ω—ã—Ö –º–∞—Å–æ–∫
            for batch_idx, original_idx in enumerate(valid_indices):
                mask_tensor = resized_batch[batch_idx]
                
                foreground_indices = torch.where(mask_tensor > 0.1)[0]
                if len(foreground_indices) == 0:
                    foreground_indices = torch.tensor([torch.argmax(mask_tensor)])
                mask_embedding = patch_tokens[foreground_indices].mean(dim=0)

                v = mask_embedding.cpu().float().numpy()
                
                # L2 –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                v_norm = np.linalg.norm(v)
                if v_norm > 1e-8:
                    v = v / v_norm
                
                # –ü—Ä–∏–≤–æ–¥–∏–º –∫ 1024 –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                if v.shape[0] != 1024:
                    out = np.zeros(1024, dtype=np.float32)
                    take = min(1024, v.shape[0])
                    out[:take] = v[:take]
                    v = out

                # –í—Å—Ç–∞–≤–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –ø–æ–∑–∏—Ü–∏—é
                while len(embeddings) <= original_idx:
                    embeddings.append(None)
                embeddings[original_idx] = v.astype(np.float32)
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—É—Å—Ç—ã–µ –º–∞—Å–∫–∏ –Ω—É–ª–µ–≤—ã–º–∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏
        for i in range(len(scaled_masks)):
            if i >= len(embeddings) or embeddings[i] is None:
                while len(embeddings) <= i:
                    embeddings.append(None)
                embeddings[i] = np.zeros(patch_tokens.shape[-1], dtype=np.float32)

        extract_time = time.time() - extract_start
        old_time_estimate = len(mask_arrays) * 0.2  # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è —Å—Ç–∞—Ä–æ–≥–æ –º–µ—Ç–æ–¥–∞
        speedup = old_time_estimate / extract_time if extract_time > 0 else 1
        print(f"   ‚ö° –ë–´–°–¢–†–û DINO: {extract_time:.3f} —Å–µ–∫ ({extract_time/len(mask_arrays)*1000:.1f} –º—Å/–º–∞—Å–∫–∞) - —É—Å–∫–æ—Ä–µ–Ω–∏–µ ~{speedup:.1f}x")

        if embeddings:
            return np.stack(embeddings, axis=0).astype(np.float32)
        return None

    def _extract_with_dino_fallback(self, image_np, mask_arrays):
        self._ensure_dino()
        if self._dino_model is None:
            return None
        import torch
        embeddings = []
        with torch.no_grad():
            for mask in mask_arrays:
                ys, xs = np.where(mask)
                if ys.size == 0 or xs.size == 0:
                    embeddings.append(np.zeros(1024, dtype=np.float32))
                    continue
                
                y1, y2 = ys.min(), ys.max()
                x1, x2 = xs.min(), xs.max()
                
                # –í—ã—Ä–µ–∑–∞–µ–º –∫—Ä–æ–ø —Å –º–∞—Å–∫–æ–π
                crop_np = image_np[y1:y2+1, x1:x2+1].copy()
                m = mask[y1:y2+1, x1:x2+1]
                if m.dtype != bool:
                    m = m.astype(bool)
                crop_np[~m] = 0
                
                # –ü–æ–ª—É—á–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –¥–ª—è –∫—Ä–æ–ø–∞, –∏—Å–ø–æ–ª—å–∑—É—è –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥
                vec = self._get_dino_global(crop_np)
                
                embeddings.append(vec)

        if embeddings:
            return np.stack(embeddings, axis=0).astype(np.float32)
        return None


    def build_queries_multiclass(self, pos_by_class, neg_imgs):
        if str(self.backbone).startswith('dinov2'):
            self._ensure_dino()
            D = 1024

            neg_list = []
            for i, img in enumerate(neg_imgs or []):
                try:
                    v = self._get_dino_global(np.array(img))
                    v = np.asarray(v, dtype=np.float32).reshape(-1)
                    v /= (np.linalg.norm(v) + 1e-8)
                    neg_list.append(v.copy())
                except Exception as e:
                    print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å negative {i}: {e}")
            q_neg = np.stack(neg_list, axis=0) if neg_list else np.zeros((0, D), dtype=np.float32)
            class_pos = {}
            if pos_by_class is None:
                pos_by_class = {}
            for cls, imgs in (pos_by_class or {}).items():
                vecs = []
                for i, img in enumerate(imgs or []):
                    try:
                        v = self._get_dino_global(np.array(img))
                        v = np.asarray(v, dtype=np.float32).reshape(-1)
                        v /= (np.linalg.norm(v) + 1e-8)
                        vecs.append(v.copy())
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å positive '{cls}' #{i}: {e}")
                Q = np.stack(vecs, axis=0) if vecs else np.zeros((0, D), dtype=np.float32)
                class_pos[cls] = Q.astype(np.float32)
                print(f"   üìä –ö–ª–∞—Å—Å '{cls}': {Q.shape[0]} –ø—Ä–∏–º–µ—Ä–æ–≤")
            print(f"   üìä Negative –≤—Å–µ–≥–æ: {q_neg.shape[0]}")
            return class_pos, q_neg.astype(np.float32)
        resnet, layer, transform, sam = (
            self.detector.searchdet_resnet,
            self.detector.searchdet_layer,
            self.detector.searchdet_transform,
            self.detector.searchdet_sam
        )

        if isinstance(resnet, tuple) and len(resnet) >= 2:
            model = resnet[0]
            device = resnet[1] if len(resnet) > 1 else 'cuda'
        else:
            model = resnet
            device = 'cuda'

        # –°–Ω–∞—á–∞–ª–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ
        neg_list = []
        embedding_dim = None
        for i, img in enumerate(neg_imgs or []):
            try:
                vec = self._get_image_embedding(img, model, layer, transform)
                if vec is None:
                    continue
                vec = np.asarray(vec, dtype=np.float32).reshape(-1)
                if embedding_dim is None: embedding_dim = vec.shape[0]
                vec /= (np.linalg.norm(vec) + 1e-8)
                neg_list.append(vec)
            except Exception as e:
                print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å negative {i}: {e}")
        
        if embedding_dim is None:
            # –ü—ã—Ç–∞–µ–º—Å—è —É–≥–∞–¥–∞—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏–∑ –ø–æ–∑–∏—Ç–∏–≤–æ–≤, –µ—Å–ª–∏ –Ω–µ–≥–∞—Ç–∏–≤–æ–≤ –Ω–µ –±—ã–ª–æ
            if pos_by_class:
                try:
                    cls, imgs = next(iter(pos_by_class.items()))
                    if imgs:
                        vec = self._get_image_embedding(imgs[0], model, layer, transform)
                        if vec is not None:
                            embedding_dim = vec.reshape(-1).shape[0]
                except Exception:
                    pass
            if embedding_dim is None:
                embedding_dim = 1024 # Fallback

        q_neg = np.stack(neg_list, axis=0) if neg_list else np.zeros((0, embedding_dim), dtype=np.float32)

        # –¢–µ–ø–µ—Ä—å –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –ø–æ –∫–ª–∞—Å—Å–∞–º
        class_pos = {}
        for cls, imgs in (pos_by_class or {}).items():
            pos_list = []
            for i, img in enumerate(imgs or []):
                try:
                    vec = self._get_image_embedding(img, model, layer, transform)
                    if vec is None:
                        continue
                    vec = np.asarray(vec, dtype=np.float32).reshape(-1)
                    vec /= (np.linalg.norm(vec) + 1e-8)
                    pos_list.append(vec)
                except Exception as e:
                    print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å positive '{cls}' #{i}: {e}")
            
            Q = np.stack(pos_list, axis=0) if pos_list else np.zeros((0, embedding_dim), dtype=np.float32)
            class_pos[cls] = Q.astype(np.float32)
            print(f"   üìä –ö–ª–∞—Å—Å '{cls}': {Q.shape[0]} –ø—Ä–∏–º–µ—Ä–æ–≤")

        print(f"   üìä Negative –≤—Å–µ–≥–æ: {q_neg.shape[0]}")
        return class_pos, q_neg


class DINOv3Embedding(nn.Module):
    def __init__(self, ckpt_path):
        super().__init__()
        self.model = ConvNeXt(arch='base', out_indices=-1)
        state_dict = torch.load(ckpt_path)['state_dict']
        self.model.load_state_dict(state_dict)
        self.model.eval()
        self.model.to(self.device)

    def preprocess(self, image):
        transform = T.Compose([
            T.Resize(256),
            T.CenterCrop(224),
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])
        return transform(image).unsqueeze(0).to(self.device)

    def get_embedding(self, x):
        with torch.no_grad():
            embedding = self.model(x)
        return embedding


class ResNet101Embedding(nn.Module):
    def __init__(self, layer='layer3'):
        super().__init__()
        self.model = ResNet(101, 'DINOv2')
        self.model.eval()
        self.model.to(self.device)

    def preprocess(self, image):
        transform = T.Compose([
            T.Resize(256),
            T.CenterCrop(224),
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])
        return transform(image).unsqueeze(0).to(self.device)

    def get_embedding(self, x):
        with torch.no_grad():
            embedding = self.model(x)
        return embedding
