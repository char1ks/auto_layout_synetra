#!/usr/bin/env python3
"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ –¥–µ—Ñ–µ–∫—Ç–æ–≤: LLaVA ‚Üí SAM2 ‚Üí OpenCV
Intelligent pipeline –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–∞ –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–µ—Ñ–µ–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é SAM2
"""

import cv2
import numpy as np
import torch
import matplotlib.pyplot as plt
from pathlib import Path
import argparse
import json
import time
from datetime import datetime
import platform
import os

# GPU Detection
def get_device():
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è –ª—É—á—à–µ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
    if torch.cuda.is_available():
        device = 'cuda'
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"üî• GPU –Ω–∞–π–¥–µ–Ω–æ: {gpu_name} ({gpu_memory:.1f}GB)")
        return device
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        print("üçé MPS (Apple Silicon) –Ω–∞–π–¥–µ–Ω–æ")
        return 'mps'
    else:
        print("üíª –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU")
        return 'cpu'

DEVICE = get_device()
print(f"üéØ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {DEVICE}")

# Platform detection
IS_WINDOWS = platform.system() == 'Windows'
IS_MACOS = platform.system() == 'Darwin'
IS_LINUX = platform.system() == 'Linux'

print(f"üñ•Ô∏è –û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞: {platform.system()}")

# LLaVA imports
try:
    from llava.model.builder import load_pretrained_model
    from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token
    from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX
    from llava.conversation import conv_templates, SeparatorStyle
    from PIL import Image
    LLAVA_AVAILABLE = True
except ImportError:
    print("‚ùå LLaVA –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –∑–∞–≥–ª—É—à–∫–∞...")
    LLAVA_AVAILABLE = False

# SAM2 imports
try:
    from sam2.build_sam import build_sam2
    from sam2.sam2_image_predictor import SAM2ImagePredictor
    from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator
    SAM2_AVAILABLE = True
    print("‚úÖ SAM2 –Ω–∞–π–¥–µ–Ω")
except ImportError as e:
    print(f"‚ùå SAM2 –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")
    SAM2_AVAILABLE = False


class MaterialAndDefectAnalyzer:
    """LLaVA –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–µ—Ñ–µ–∫—Ç–æ–≤"""
    
    def __init__(self, model_path="liuhaotian/llava-v1.5-7b"):
        self.model_path = model_path
        
        # –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ —É—Ä–æ–≤–Ω—é –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
        self.available_models = {
            "detailed": "liuhaotian/llava-v1.5-13b",  # 13B - –æ—á–µ–Ω—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            "standard": "liuhaotian/llava-v1.5-7b",   # 7B - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            "latest": "liuhaotian/llava-v1.6-vicuna-13b",  # –ù–æ–≤–µ–π—à–∞—è 13B –≤–µ—Ä—Å–∏—è
            "onevision": "lmms-lab/llava-onevision-qwen2-7b-ov"  # –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        }
        self.model = None
        self.tokenizer = None
        self.image_processor = None
        self.context_len = None
        
        if LLAVA_AVAILABLE:
            self._load_model()
    
    def switch_model(self, model_type="detailed"):
        """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å LLaVA –¥–ª—è —Ä–∞–∑–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏"""
        if model_type in self.available_models:
            print(f"üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –º–æ–¥–µ–ª—å: {model_type}")
            self.model_path = self.available_models[model_type]
            self._load_model()
        else:
            print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {model_type}")
            print(f"‚úÖ –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–∏–ø—ã: {list(self.available_models.keys())}")
    
    def get_model_info(self):
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏"""
        return {
            "current_model": self.model_path,
            "available_models": self.available_models,
            "model_loaded": self.model is not None
        }
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ LLaVA"""
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º LLaVA –º–æ–¥–µ–ª—å...")
        
        # –§–ò–ö–° –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        import torch
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        
        try:
            self.model_name = get_model_name_from_path(self.model_path)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            is_large_model = "13b" in self.model_path.lower() or "34b" in self.model_path.lower()
            
            print(f"üìä –ú–æ–¥–µ–ª—å: {self.model_path}")
            print(f"üìè –ë–æ–ª—å—à–∞—è –º–æ–¥–µ–ª—å: {is_large_model}")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –∏ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏
            if DEVICE == 'cuda':
                # CUDA - –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π
                if is_large_model:
                    print("üîß –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏...")
                    self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(
                        self.model_path, None, self.model_name, 
                        device_map="auto",
                        load_8bit=True,  # 8bit –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ –Ω–∞ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö
                        load_4bit=False,
                        torch_dtype=torch.float16
                    )
                else:
                    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–ª—è 7B –º–æ–¥–µ–ª–∏
                    self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(
                        self.model_path, None, self.model_name, 
                        device_map="auto",
                        load_8bit=False, load_4bit=False,
                        torch_dtype=torch.float16
                    )
                    self.model = self.model.half()
                
            elif DEVICE == 'mps':
                # Apple Silicon
                self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(
                    self.model_path, None, self.model_name,
                    device_map="mps",
                    load_8bit=False, load_4bit=False,
                    torch_dtype=torch.float16
                )
                self.model = self.model.half()
            else:
                # CPU fallback
                self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(
                    self.model_path, None, self.model_name,
                    device_map="cpu",
                    load_8bit=False, load_4bit=False,
                    torch_dtype=torch.float32  # float32 –¥–ª—è CPU
                )
                self.model = self.model.float()
            
            print(f"‚úÖ LLaVA –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {DEVICE}")
            print("‚úÖ LLaVA –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ LLaVA: {e}")
            print("üîÑ –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å —É–ø—Ä–æ—â–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏...")
            try:
                # –í—Ç–æ—Ä–æ–π —Å–ø–æ—Å–æ–± - —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
                if DEVICE == 'cuda':
                    dtype = torch.float16
                else:
                    dtype = torch.float32
                    
                self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(
                    self.model_path, None, self.model_name, torch_dtype=dtype
                )
                self.model = self.model.to(DEVICE)
                if DEVICE == 'cpu':
                    self.model = self.model.float()
                print("‚úÖ LLaVA –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å —É–ø—Ä–æ—â–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏")
            except Exception as e2:
                print(f"‚ùå –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ LLaVA: {e2}")
                self.model = None
    
    def classify_material(self, image_path):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"""
        if not LLAVA_AVAILABLE or self.model is None:
            # –ó–∞–≥–ª—É—à–∫–∞
            print("‚ö†Ô∏è LLaVA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É")
            return {
                "material": "metal", 
                "confidence": 0.8,
                "description": "metallic surface with potential defects"
            }
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image = Image.open(image_path).convert('RGB')
            
            # –ü—Ä–æ–º–ø—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞
            prompt = """Look at this image carefully. What type of material is shown? 
            Choose from: metal, wood, plastic, concrete, fabric, glass, ceramic.
            Also describe the surface condition and overall quality.
            
            Format your answer as:
            Material: [material_type]
            Condition: [surface_description]
            Quality: [overall_assessment]"""
            
            response = self._get_llava_response(image, prompt)
            material_type = self._parse_material(response)
            
            return {
                "material": material_type,
                "confidence": 0.9,
                "description": response
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞: {e}")
            return {
                "material": "unknown", 
                "confidence": 0.5,
                "description": "failed to analyze"
            }
    
    def analyze_defects(self, image_path, material_type="unknown"):
        """–î–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–µ—Ñ–µ–∫—Ç–æ–≤: —Å–Ω–∞—á–∞–ª–∞ –Ω–∞–π—Ç–∏ –æ–±—ä–µ–∫—Ç—ã —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏, –ø–æ—Ç–æ–º –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–µ—Ñ–µ–∫—Ç—ã"""
        if not LLAVA_AVAILABLE or self.model is None:
            # –ó–∞–≥–ª—É—à–∫–∞
            print("‚ö†Ô∏è LLaVA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–µ—Ñ–µ–∫—Ç–æ–≤")
            return {
                "defects_found": True,
                "defect_types": ["scratch", "dent", "missing_part"],
                "defect_locations": ["center", "top-right", "edge"],
                "severity": "moderate",
                "completeness": "incomplete",
                "description": "Detected potential surface defects and structural issues",
                "bounding_boxes": [(0.2, 0.2, 0.8, 0.8)],  # x1, y1, x2, y2 (normalized)
                "prompt_points": [[320, 240], [400, 150], [200, 300]]
            }
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image = Image.open(image_path).convert('RGB')
            
            # –≠–¢–ê–ü 1: –ù–∞–π—Ç–∏ –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã —Å —Ç–æ—á–Ω—ã–º–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏
            print("üîç –≠—Ç–∞–ø 1: –ü–æ–∏—Å–∫ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –∏—Ö –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è...")
            object_detection_result = self._detect_objects_with_coordinates(image, material_type)
            
            # –≠–¢–ê–ü 2: –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞
            print("üî¨ –≠—Ç–∞–ø 2: –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤...")
            defect_analysis_result = self._analyze_objects_for_defects(image, object_detection_result, material_type)
            
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            combined_result = {
                **defect_analysis_result,
                "bounding_boxes": object_detection_result.get("bounding_boxes", []),
                "objects_found": object_detection_result.get("objects_found", [])
            }
            
            return combined_result
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–µ—Ñ–µ–∫—Ç–æ–≤: {e}")
            return {
                "defects_found": False,
                "defect_types": [],
                "defect_locations": [],
                "severity": "unknown",
                "description": "failed to analyze defects",
                "bounding_boxes": [],
                "prompt_points": []
            }
    
    def _detect_objects_with_coordinates(self, image, material_type):
        """–≠–¢–ê–ü 1: –ü–æ–∏—Å–∫ –æ–±—ä–µ–∫—Ç–æ–≤ —Å —Ç–æ—á–Ω—ã–º–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ bounding box"""
        
        # –ü—Ä–æ–º–ø—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–±—ä–µ–∫—Ç–æ–≤ —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏
        detection_prompt = f"""OBJECT DETECTION TASK: Analyze this {material_type} image and identify ALL visible objects/components with their EXACT locations.

        For each object you see, provide:
        1. Object type (wire, cable, connector, screw, pin, contact, etc.)
        2. Bounding box coordinates in format: (x1, y1, x2, y2)
        3. Brief description of the object
        
        COORDINATE FORMAT:
        - Use normalized coordinates (0.0 to 1.0)
        - (x1, y1) = top-left corner
        - (x2, y2) = bottom-right corner
        - Example: wire at top-left would be (0.1, 0.1, 0.4, 0.3)
        
        FOCUS ON FINDING:
        - Individual wires and wire strands
        - Cables and cable bundles
        - Connectors, pins, terminals
        - Screws, bolts, fasteners
        - Electronic components
        - Any damaged or missing areas
        
        OUTPUT FORMAT:
        Object: wire_strand_1, Box: (0.2, 0.3, 0.25, 0.4), Description: thin copper wire
        Object: connector_pin, Box: (0.5, 0.1, 0.55, 0.15), Description: metal contact pin
        Object: missing_area, Box: (0.7, 0.6, 0.8, 0.7), Description: gap where component should be
        
        Be very precise with coordinates. Look for EVERYTHING, including tiny details."""
        
        response = self._get_llava_response(image, detection_prompt)
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –æ–±—ä–µ–∫—Ç–æ–≤
        return self._parse_object_coordinates(response)
    
    def _analyze_objects_for_defects(self, image, detection_result, material_type):
        """–≠–¢–ê–ü 2: –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –¥–µ—Ñ–µ–∫—Ç–æ–≤"""
        
        objects_info = "\n".join([f"- {obj['type']}: {obj['description']} at {obj['box']}" 
                                 for obj in detection_result.get("objects_found", [])])
        
        analysis_prompt = f"""DEFECT ANALYSIS: Based on the detected objects below, analyze each one for defects and issues.

        DETECTED OBJECTS:
        {objects_info}
        
        CRITICAL ANALYSIS FOR {material_type.upper()}:
        
        **WIRE & CABLE INSPECTION:**
        - Count individual wire strands in each cable
        - Check if any copper wires are missing from bundles
        - Look for exposed/protruding wire strands
        - Verify insulation integrity
        - Check wire routing and positioning
        
        **CONNECTOR ANALYSIS:**
        - Verify all pins/contacts are present
        - Check for bent or damaged pins
        - Look for corrosion or oxidation
        - Verify proper alignment and seating
        
        **STRUCTURAL INSPECTION:**
        - Identify missing components or fasteners
        - Check for cracks, breaks, or deformation
        - Look for incomplete assemblies
        - Verify proper component orientation
        
        **SURFACE EXAMINATION:**
        - Detect scratches, dents, wear patterns
        - Look for discoloration or staining
        - Check for coating or paint damage
        
        PROVIDE DETAILED ANALYSIS:
        1. Are there visible defects? (Yes/No)
        2. What specific defects do you see for each object?
        3. For wires: Are any strands missing or protruding?
        4. For connectors: Are all pins present and undamaged?
        5. Rate severity: minor/moderate/severe/critical
        6. Is the assembly complete or incomplete?
        7. Describe the overall condition and any missing parts
        
        Focus on identifying subtle issues like individual missing wire strands or slightly bent pins."""
        
        response = self._get_llava_response(image, analysis_prompt)
        
        # –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–µ—Ñ–µ–∫—Ç–æ–≤
        return self._parse_defect_analysis(response, image.size)
    
    def _parse_object_coordinates(self, response):
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLaVA"""
        import re
        
        objects_found = []
        bounding_boxes = []
        
        # –ü–æ–∏—Å–∫ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ: Object: name, Box: (x1, y1, x2, y2), Description: desc
        pattern = r'Object:\s*([^,]+),\s*Box:\s*\(([^)]+)\),\s*Description:\s*(.+)'
        matches = re.findall(pattern, response, re.IGNORECASE)
        
        for match in matches:
            object_type = match[0].strip()
            coords_str = match[1].strip()
            description = match[2].strip()
            
            try:
                # –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
                coords = [float(x.strip()) for x in coords_str.split(',')]
                if len(coords) == 4:
                    x1, y1, x2, y2 = coords
                    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0-1)
                    if all(0 <= coord <= 1 for coord in coords) and x2 > x1 and y2 > y1:
                        objects_found.append({
                            "type": object_type,
                            "box": (x1, y1, x2, y2),
                            "description": description
                        })
                        bounding_boxes.append((x1, y1, x2, y2))
            except (ValueError, IndexError):
                continue
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –æ–±—ä–µ–∫—Ç—ã –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ª—é–±—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
        if not bounding_boxes:
            coord_pattern = r'\(([0-9.]+),\s*([0-9.]+),\s*([0-9.]+),\s*([0-9.]+)\)'
            coord_matches = re.findall(coord_pattern, response)
            
            for match in coord_matches:
                try:
                    coords = [float(x) for x in match]
                    if all(0 <= coord <= 1 for coord in coords) and coords[2] > coords[0] and coords[3] > coords[1]:
                        bounding_boxes.append(tuple(coords))
                        objects_found.append({
                            "type": "detected_object",
                            "box": tuple(coords),
                            "description": "Object detected from coordinates"
                        })
                except ValueError:
                    continue
        
        return {
            "objects_found": objects_found,
            "bounding_boxes": bounding_boxes,
            "detection_response": response
        }
    
    def _get_llava_response(self, image, prompt):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLaVA"""
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        conv = conv_templates["vicuna_v1"].copy()
        conv.append_message(conv.roles[0], f"{DEFAULT_IMAGE_TOKEN}\n{prompt}")
        conv.append_message(conv.roles[1], None)
        prompt_formatted = conv.get_prompt()
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å padding
        input_ids = tokenizer_image_token(prompt_formatted, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt')
        if input_ids.dim() == 1:
            input_ids = input_ids.unsqueeze(0)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ç–∏–ø–æ–º –¥–∞–Ω–Ω—ã—Ö
        image_tensor = process_images([image], self.image_processor, self.model.config)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        if DEVICE == 'cuda':
            target_dtype = torch.float16
        else:
            target_dtype = torch.float32
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ–º –∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ç–∏–ø–æ–º –¥–∞–Ω–Ω—ã—Ö
        input_ids = input_ids.to(DEVICE)
        if hasattr(image_tensor, 'to'):
            image_tensor = image_tensor.to(DEVICE, dtype=target_dtype)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –∏ –º–æ–¥–µ–ª–∏
        is_large_model = "13b" in self.model_path.lower() or "34b" in self.model_path.lower()
        
        if DEVICE == 'cuda':
            max_tokens = 1024 if is_large_model else 512  # –ë–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        else:
            max_tokens = 512 if is_large_model else 256
        
        with torch.inference_mode():
            output_ids = self.model.generate(
                input_ids,
                images=image_tensor,
                image_sizes=[image.size],
                do_sample=True,
                temperature=0.2,
                max_new_tokens=max_tokens,
                use_cache=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        output = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]
        response = output.split("ASSISTANT:")[-1].strip()
        
        return response
    
    def _parse_material(self, response):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∏–ø–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–∞ –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLaVA"""
        response_lower = response.lower()
        
        materials = ["metal", "wood", "plastic", "concrete", "fabric", "glass", "ceramic"]
        
        for material in materials:
            if material in response_lower:
                return material
        
        # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        if any(word in response_lower for word in ["steel", "iron", "aluminum", "copper"]):
            return "metal"
        elif any(word in response_lower for word in ["wooden", "timber", "oak", "pine"]):
            return "wood"
        elif any(word in response_lower for word in ["rubber", "polymer"]):
            return "plastic"
        
        return "unknown"
    
    def _parse_defect_analysis(self, response, image_size):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–µ—Ñ–µ–∫—Ç–æ–≤"""
        response_lower = response.lower()
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–ª–∏—á–∏—è –¥–µ—Ñ–µ–∫—Ç–æ–≤/–ø—Ä–æ–±–ª–µ–º
        defects_found = any(word in response_lower for word in [
            "yes", "defect", "damage", "scratch", "crack", "dent", "stain", 
            "missing", "broken", "incomplete", "separated", "detached", "bent"
        ])
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ç–∏–ø—ã –¥–µ—Ñ–µ–∫—Ç–æ–≤ –∏ –ø—Ä–æ–±–ª–µ–º (–≤–∫–ª—é—á–∞—è –º–µ–ª–∫–∏–µ –¥–µ—Ç–∞–ª–∏)
        defect_types = []
        defect_keywords = {
            # –ü–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–µ –¥–µ—Ñ–µ–∫—Ç—ã
            "scratch": ["scratch", "scratches", "scrape", "scraping"],
            "crack": ["crack", "cracks", "fracture", "split", "tear"],
            "dent": ["dent", "dents", "deformation", "depression"],
            "corrosion": ["rust", "corrosion", "oxidation", "rusted"],
            "stain": ["stain", "discoloration", "spot", "mark"],
            "wear": ["wear", "worn", "erosion", "abraded"],
            "chip": ["chip", "chips", "chipping", "flaking"],
            
            # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
            "missing_part": ["missing", "absent", "lost", "hole where", "should be"],
            "broken_off": ["broken off", "detached", "separated", "fell off", "torn off"],
            "bent": ["bent", "twisted", "warped", "deformed", "curved"],
            "incomplete": ["incomplete", "unfinished", "partial", "half"],
            "loose": ["loose", "wobbling", "unstable", "not secure"],
            "gap": ["gap", "space", "opening", "separation"],
            "asymmetry": ["asymmetric", "uneven", "lopsided", "misaligned"],
            
            # –ü—Ä–æ–±–ª–µ–º—ã —Å –ø—Ä–æ–≤–æ–¥–∞–º–∏ –∏ –∫–∞–±–µ–ª—è–º–∏ (–ù–û–í–û–ï!)
            "wire_missing": ["missing wire", "wire missing", "missing strand", "strand missing", "wire absent"],
            "wire_exposed": ["exposed wire", "wire sticking", "protruding wire", "wire out", "copper showing"],
            "wire_frayed": ["frayed wire", "frayed", "wire broken", "damaged wire", "torn wire"],
            "wire_misrouted": ["misrouted", "wrong position", "incorrect routing", "wire placement"],
            "insulation_damage": ["damaged insulation", "insulation broken", "bare copper", "exposed copper"],
            
            # –ü—Ä–æ–±–ª–µ–º—ã —Å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è–º–∏ –∏ –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏
            "connector_issue": ["missing pin", "pin missing", "connector damage", "contact issue"],
            "solder_defect": ["cold joint", "solder crack", "poor solder", "excess solder"],
            "contact_corrosion": ["corroded contact", "contact corrosion", "oxidized contact"],
            "misalignment": ["misaligned", "not aligned", "crooked", "tilted"],
            
            # –ú–µ–ª–∫–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
            "tiny_missing": ["tiny", "small missing", "micro component", "fastener missing"],
            "assembly_error": ["not seated", "not inserted", "wrong orientation", "upside down"],
            
            # –ü—Ä–æ–±–ª–µ–º—ã —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏
            "fracture": ["fractured", "broken", "split", "cracked through"],
            "edge_damage": ["edge damage", "torn edge", "damaged edge"],
            "dimensional": ["wrong size", "distorted", "out of shape"]
        }
        
        for defect_type, keywords in defect_keywords.items():
            if any(keyword in response_lower for keyword in keywords):
                defect_types.append(defect_type)
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ª–æ–∫–∞—Ü–∏–∏ –¥–µ—Ñ–µ–∫—Ç–æ–≤
        defect_locations = []
        location_keywords = {
            "top-left": ["top-left", "top left", "upper left", "corner top-left"],
            "top-center": ["top-center", "top center", "upper center", "top"],
            "top-right": ["top-right", "top right", "upper right", "corner top-right"],
            "center-left": ["center-left", "center left", "middle left", "left"],
            "center": ["center", "middle", "central", "middle area"],
            "center-right": ["center-right", "center right", "middle right", "right"],
            "bottom-left": ["bottom-left", "bottom left", "lower left", "corner bottom-left"],
            "bottom-center": ["bottom-center", "bottom center", "lower center", "bottom"],
            "bottom-right": ["bottom-right", "bottom right", "lower right", "corner bottom-right"],
            "edge": ["edge", "rim", "border", "perimeter"],
            "corner": ["corner", "angle", "joint"],
            "multiple": ["multiple", "several", "various", "throughout"]
        }
        
        for location, keywords in location_keywords.items():
            if any(keyword in response_lower for keyword in keywords):
                defect_locations.append(location)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏ (–≤–∫–ª—é—á–∞—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å)
        severity = "unknown"
        if any(word in response_lower for word in ["critical", "catastrophic", "complete failure", "totally broken"]):
            severity = "critical"
        elif any(word in response_lower for word in ["severe", "serious", "major", "significant", "extensive"]):
            severity = "severe"
        elif any(word in response_lower for word in ["moderate", "medium", "noticeable", "considerable"]):
            severity = "moderate"
        elif any(word in response_lower for word in ["minor", "small", "slight", "light", "superficial"]):
            severity = "minor"
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Ç—ã –æ–±—ä–µ–∫—Ç–∞
        completeness = "unknown"
        if any(word in response_lower for word in ["complete", "intact", "whole", "all parts present"]):
            completeness = "complete"
        elif any(word in response_lower for word in ["incomplete", "missing parts", "partial", "broken off"]):
            completeness = "incomplete"
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ—á–µ–∫-–ø–æ–¥—Å–∫–∞–∑–æ–∫ –¥–ª—è SAM2 –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–æ–∫–∞—Ü–∏–π
        prompt_points = self._generate_prompt_points(defect_locations, image_size)
        
        return {
            "defects_found": defects_found,
            "defect_types": defect_types,
            "defect_locations": defect_locations,
            "severity": severity,
            "completeness": completeness,
            "description": response,
            "prompt_points": prompt_points
        }
    
    def _generate_prompt_points(self, locations, image_size):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ—á–µ–∫-–ø–æ–¥—Å–∫–∞–∑–æ–∫ –¥–ª—è SAM2 –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ª–æ–∫–∞—Ü–∏–π"""
        width, height = image_size
        prompt_points = []
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∫–∞—Ä—Ç–∞ –ª–æ–∫–∞—Ü–∏–π –≤ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ)
        location_map = {
            "top-left": (0.25, 0.25),
            "top-center": (0.5, 0.25),
            "top-right": (0.75, 0.25),
            "center-left": (0.25, 0.5),
            "center": (0.5, 0.5),
            "center-right": (0.75, 0.5),
            "bottom-left": (0.25, 0.75),
            "bottom-center": (0.5, 0.75),
            "bottom-right": (0.75, 0.75),
            "edge": (0.5, 0.1),  # –í–µ—Ä—Ö–Ω–∏–π –∫—Ä–∞–π
            "corner": (0.9, 0.1),  # –ü—Ä–∞–≤—ã–π –≤–µ—Ä—Ö–Ω–∏–π —É–≥–æ–ª
        }
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ª–æ–∫–∞—Ü–∏–π
        if "multiple" in locations:
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–æ—á–µ–∫ –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–µ—Ñ–µ–∫—Ç–æ–≤
            multiple_points = [
                (0.3, 0.3), (0.7, 0.3), (0.3, 0.7), (0.7, 0.7), (0.5, 0.5)
            ]
            for rel_x, rel_y in multiple_points:
                x = int(rel_x * width)
                y = int(rel_y * height)
                prompt_points.append([x, y])
        
        for location in locations:
            if location in location_map:
                rel_x, rel_y = location_map[location]
                x = int(rel_x * width)
                y = int(rel_y * height)
                prompt_points.append([x, y])
        
        # –ï—Å–ª–∏ –ª–æ–∫–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –¥–æ–±–∞–≤–ª—è–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É—é —Ç–æ—á–∫—É
        if not prompt_points:
            prompt_points.append([width // 2, height // 2])
        
        return prompt_points
    
    def _generate_points_from_boxes(self, bounding_boxes, image_size):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ—á–µ–∫ –∏–∑ bounding boxes –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤"""
        width, height = image_size
        points = []
        
        for bbox in bounding_boxes:
            x1_norm, y1_norm, x2_norm, y2_norm = bbox
            
            # –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ bounding box
            center_x = int((x1_norm + x2_norm) / 2 * width)
            center_y = int((y1_norm + y2_norm) / 2 * height)
            points.append([center_x, center_y])
            
            # –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–æ—á–µ–∫ –≤–Ω—É—Ç—Ä–∏ box –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
            box_width = (x2_norm - x1_norm) * width
            box_height = (y2_norm - y1_norm) * height
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤
            if box_width > 50 and box_height > 50:
                # –ß–µ—Ç—ã—Ä–µ —Ç–æ—á–∫–∏ –≤ –∫–≤–∞–¥—Ä–∞–Ω—Ç–∞—Ö
                quad_points = [
                    [int((x1_norm + (x2_norm - x1_norm) * 0.3) * width), 
                     int((y1_norm + (y2_norm - y1_norm) * 0.3) * height)],
                    [int((x1_norm + (x2_norm - x1_norm) * 0.7) * width), 
                     int((y1_norm + (y2_norm - y1_norm) * 0.3) * height)],
                    [int((x1_norm + (x2_norm - x1_norm) * 0.3) * width), 
                     int((y1_norm + (y2_norm - y1_norm) * 0.7) * height)],
                    [int((x1_norm + (x2_norm - x1_norm) * 0.7) * width), 
                     int((y1_norm + (y2_norm - y1_norm) * 0.7) * height)]
                ]
                points.extend(quad_points)
        
        return points


class SAM2DefectSegmenter:
    """SAM2 –¥–ª—è —Ç–æ—á–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–µ—Ñ–µ–∫—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–¥—Å–∫–∞–∑–æ–∫ –æ—Ç LLaVA"""
    
    def __init__(self):
        self.predictor = None
        self.mask_generator = None
        
        if SAM2_AVAILABLE:
            self._load_model()
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ SAM2"""
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º SAM2 –º–æ–¥–µ–ª—å...")
        try:
            import os
            import urllib.request
            
            # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
            models_dir = Path("./models")
            models_dir.mkdir(exist_ok=True)
            
            config_name = "sam2_hiera_l.yaml"
            checkpoint_path = models_dir / "sam2_hiera_large.pt"
            
            # –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
            if not checkpoint_path.exists():
                print("üì• –°–∫–∞—á–∏–≤–∞–µ–º SAM2 –º–æ–¥–µ–ª—å (—ç—Ç–æ –∑–∞–π–º–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç)...")
                model_url = "https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt"
                
                try:
                    urllib.request.urlretrieve(model_url, checkpoint_path)
                    print(f"‚úÖ SAM2 –º–æ–¥–µ–ª—å —Å–∫–∞—á–∞–Ω–∞: {checkpoint_path}")
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è SAM2: {e}")
                    print("üîÑ –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å torch.hub...")
                    # Fallback: –ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ torch.hub
                    import torch
                    checkpoint_path = torch.hub.load_state_dict_from_url(
                        "https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt",
                        model_dir=str(models_dir)
                    )
            
            print(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º SAM2: {config_name} + {checkpoint_path}")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ SAM2
            sam2_model = build_sam2(config_name, str(checkpoint_path), device=DEVICE)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞ –¥–ª—è —Ç–æ—á–µ—á–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫
            self.predictor = SAM2ImagePredictor(sam2_model)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –º–∞—Å–æ–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            self.mask_generator = SAM2AutomaticMaskGenerator(
                model=sam2_model,
                points_per_side=32,
                pred_iou_thresh=0.7,
                stability_score_thresh=0.85,
                crop_n_layers=1,
                crop_n_points_downscale_factor=2,
                min_mask_region_area=100
            )
            
            print("‚úÖ SAM2 –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ SAM2: {e}")
            print("üîÑ –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± –∑–∞–≥—Ä—É–∑–∫–∏...")
            
            try:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–±: –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É
                print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é SAM2...")
                sam2_model = build_sam2("sam2_hiera_s.yaml", "sam2_hiera_small.pt", device=DEVICE)
                
                # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞ –¥–ª—è —Ç–æ—á–µ—á–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫
                self.predictor = SAM2ImagePredictor(sam2_model)
                
                # –°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –º–∞—Å–æ–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
                self.mask_generator = SAM2AutomaticMaskGenerator(
                    model=sam2_model,
                    points_per_side=16,  # –ú–µ–Ω—å—à–µ —Ç–æ—á–µ–∫ –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è
                    pred_iou_thresh=0.7,
                    stability_score_thresh=0.85,
                    crop_n_layers=1,
                    crop_n_points_downscale_factor=2,
                    min_mask_region_area=100
                )
                
                print("‚úÖ SAM2 —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            except Exception as e2:
                print(f"‚ùå –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ SAM2: {e2}")
                print("‚ö†Ô∏è SAM2 –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è")
                self.predictor = None
                self.mask_generator = None
    
    def segment_defects_with_prompts(self, image_path, defect_analysis):
        """–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–µ—Ñ–µ–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–æ–¥—Å–∫–∞–∑–æ–∫ –æ—Ç LLaVA"""
        if not SAM2_AVAILABLE or self.predictor is None:
            print("‚ö†Ô∏è SAM2 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é")
            return self._simple_segmentation(image_path)
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image = cv2.imread(str(image_path))
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä
            self.predictor.set_image(image_rgb)
            
            all_masks = []
            
            # –ü–†–ò–û–†–ò–¢–ï–¢ 1: –ò—Å–ø–æ–ª—å–∑—É–µ–º bounding boxes –æ—Ç LLaVA (—Å–∞–º—ã–µ —Ç–æ—á–Ω—ã–µ)
            if defect_analysis.get("bounding_boxes"):
                print(f"üì¶ –ò—Å–ø–æ–ª—å–∑—É–µ–º {len(defect_analysis['bounding_boxes'])} bounding boxes –æ—Ç LLaVA")
                
                height, width = image_rgb.shape[:2]
                
                for i, bbox in enumerate(defect_analysis["bounding_boxes"]):
                    try:
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –≤ –ø–∏–∫—Å–µ–ª–∏
                        x1_norm, y1_norm, x2_norm, y2_norm = bbox
                        x1 = int(x1_norm * width)
                        y1 = int(y1_norm * height)
                        x2 = int(x2_norm * width)
                        y2 = int(y2_norm * height)
                        
                        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
                        x1, x2 = max(0, min(x1, x2)), min(width-1, max(x1, x2))
                        y1, y2 = max(0, min(y1, y2)), min(height-1, max(y1, y2))
                        
                        if x2 > x1 and y2 > y1:
                            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–∞—Å–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º bounding box
                            input_box = np.array([x1, y1, x2, y2])
                            
                            masks, scores, logits = self.predictor.predict(
                                box=input_box[None, :],  # Box prompt
                                multimask_output=False
                            )
                            
                            # –ü–æ–ª—É—á–∞–µ–º –º–∞—Å–∫—É
                            mask = masks[0]
                            mask_uint8 = (mask * 255).astype(np.uint8)
                            all_masks.append(mask_uint8)
                            
                            print(f"   ‚úÖ Box {i+1}: —Ç–æ—á–Ω–æ—Å—Ç—å {scores[0]:.3f}, –æ–±–ª–∞—Å—Ç—å ({x1},{y1})-({x2},{y2})")
                        
                    except Exception as e:
                        print(f"   ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ box {i+1}: {e}")
                        continue
            
            # –ü–†–ò–û–†–ò–¢–ï–¢ 2: –ï—Å–ª–∏ –Ω–µ—Ç bounding boxes, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—á–∫–∏
            elif defect_analysis.get("prompt_points"):
                print(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º {len(defect_analysis['prompt_points'])} —Ç–æ—á–µ–∫-–ø–æ–¥—Å–∫–∞–∑–æ–∫ –æ—Ç LLaVA")
                
                for i, point in enumerate(defect_analysis["prompt_points"]):
                    try:
                        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–∞—Å–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏
                        masks, scores, logits = self.predictor.predict(
                            point_coords=np.array([point]),
                            point_labels=np.array([1]),  # 1 = foreground
                            multimask_output=True
                        )
                        
                        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–∞—Å–∫—É
                        best_mask_idx = np.argmax(scores)
                        best_mask = masks[best_mask_idx]
                        
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ uint8
                        mask_uint8 = (best_mask * 255).astype(np.uint8)
                        all_masks.append(mask_uint8)
                        
                        print(f"   ‚úÖ –¢–æ—á–∫–∞ {i+1}: —Ç–æ—á–Ω–æ—Å—Ç—å {scores[best_mask_idx]:.3f}")
                        
                    except Exception as e:
                        print(f"   ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ—á–∫–∏ {i+1}: {e}")
                        continue
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é
            if defect_analysis.get("defects_found", False):
                print("ü§ñ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è...")
                try:
                    auto_masks = self.mask_generator.generate(image_rgb)
                    
                    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –º–∞—Å–æ–∫
                    auto_masks_sorted = sorted(auto_masks, key=lambda x: x['predicted_iou'], reverse=True)
                    
                    for auto_mask in auto_masks_sorted[:5]:  # –ú–∞–∫—Å–∏–º—É–º 5 –ª—É—á—à–∏—Ö
                        mask = auto_mask['segmentation']
                        mask_uint8 = (mask * 255).astype(np.uint8)
                        all_masks.append(mask_uint8)
                
                except Exception as e:
                    print(f"   ‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {e}")
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –º–∞—Å–æ–∫
            filtered_masks = self._filter_similar_masks(all_masks)
            
            return {
                "masks": filtered_masks,
                "num_detections": len(filtered_masks),
                "defect_analysis": defect_analysis
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ SAM2: {e}")
            return self._simple_segmentation(image_path)
    
    def _filter_similar_masks(self, masks, iou_threshold=0.7):
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ—Ö–æ–∂–∏—Ö –º–∞—Å–æ–∫"""
        if len(masks) <= 1:
            return masks
        
        filtered = []
        used_indices = set()
        
        for i, mask1 in enumerate(masks):
            if i in used_indices:
                continue
            
            is_unique = True
            for j, mask2 in enumerate(masks[i+1:], i+1):
                if j in used_indices:
                    continue
                
                # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ IoU
                intersection = cv2.bitwise_and(mask1, mask2)
                union = cv2.bitwise_or(mask1, mask2)
                
                intersection_area = cv2.countNonZero(intersection)
                union_area = cv2.countNonZero(union)
                
                if union_area > 0:
                    iou = intersection_area / union_area
                    if iou > iou_threshold:
                        # –û—Å—Ç–∞–≤–ª—è–µ–º –º–∞—Å–∫—É —Å –±–æ–ª—å—à–µ–π –ø–ª–æ—â–∞–¥—å—é
                        area1 = cv2.countNonZero(mask1)
                        area2 = cv2.countNonZero(mask2)
                        if area2 > area1:
                            is_unique = False
                            break
                        else:
                            used_indices.add(j)
            
            if is_unique:
                filtered.append(mask1)
                used_indices.add(i)
        
        return filtered
    
    def _simple_segmentation(self, image_path):
        """–ü—Ä–æ—Å—Ç–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è —Å–ª—É—á–∞—è –∫–æ–≥–¥–∞ SAM2 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
        image = cv2.imread(str(image_path))
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # –ü—Ä–æ—Å—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
        
        # –ü–æ–∏—Å–∫ –∫–æ–Ω—Ç—É—Ä–æ–≤
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å–æ–∫ –∏–∑ –∫–æ–Ω—Ç—É—Ä–æ–≤
        masks = []
        for contour in contours:
            if cv2.contourArea(contour) > 100:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä
                mask = np.zeros_like(gray)
                cv2.fillPoly(mask, [contour], 255)
                masks.append(mask)
        
        return {
            "masks": masks[:10],  # –ú–∞–∫—Å–∏–º—É–º 10 –º–∞—Å–æ–∫
            "num_detections": len(masks[:10]),
            "defect_analysis": {"defects_found": True, "description": "Simple segmentation fallback"}
        }


class OpenCVPostProcessor:
    """OpenCV –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    
    def __init__(self):
        self.material_params = {
            "metal": {"min_area": 100, "max_area": 10000, "morph_kernel": 3},
            "wood": {"min_area": 200, "max_area": 15000, "morph_kernel": 5},
            "plastic": {"min_area": 50, "max_area": 8000, "morph_kernel": 2},
            "default": {"min_area": 100, "max_area": 12000, "morph_kernel": 3}
        }
    
    def process_masks(self, image, masks, material_type="default"):
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–∞—Å–æ–∫ —Å —É—á–µ—Ç–æ–º —Ç–∏–ø–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–∞"""
        if not masks:
            return []
        
        params = self.material_params.get(material_type, self.material_params["default"])
        processed_masks = []
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        target_height, target_width = image.shape[:2]
        
        for mask in masks:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –º–∞—Å–∫–∏
            if mask.shape[:2] != (target_height, target_width):
                # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –º–∞—Å–∫–∏ –ø–æ–¥ –∏—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                mask = cv2.resize(mask, (target_width, target_height), interpolation=cv2.INTER_NEAREST)
            
            processed_mask = self._clean_mask(mask, params)
            if processed_mask is not None:
                processed_masks.append(processed_mask)
        
        return processed_masks
    
    def _clean_mask(self, mask, params):
        """–û—á–∏—Å—Ç–∫–∞ –æ–¥–Ω–æ–π –º–∞—Å–∫–∏"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –±–∏–Ω–∞—Ä–Ω—É—é –º–∞—Å–∫—É
            if mask.dtype != np.uint8:
                mask = (mask * 255).astype(np.uint8)
            
            # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (params["morph_kernel"], params["morph_kernel"]))
            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
            area = cv2.countNonZero(mask)
            if params["min_area"] <= area <= params["max_area"]:
                return mask
            
            return None
        except:
            return None


class DefectAnalysisPipeline:
    """–û—Å–Ω–æ–≤–Ω–æ–π pipeline –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–µ—Ñ–µ–∫—Ç–æ–≤: LLaVA ‚Üí SAM2 ‚Üí OpenCV"""
    
    def __init__(self, model_type="detailed"):
        self.analyzer = MaterialAndDefectAnalyzer()
        if model_type != "detailed":
            self.analyzer.switch_model(model_type)
        self.segmenter = SAM2DefectSegmenter()
        self.postprocessor = OpenCVPostProcessor()
    
    def switch_model(self, model_type):
        """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ LLaVA –¥–ª—è —Ä–∞–∑–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏"""
        self.analyzer.switch_model(model_type)
    
    def get_model_info(self):
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏"""
        return self.analyzer.get_model_info()
    
    def analyze_image(self, image_path, output_dir="./output"):
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        print(f"\nüîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}")
        start_time = time.time()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "image_path": str(image_path),
            "stages": {}
        }
        
        try:
            # –≠—Ç–∞–ø 1: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞
            print("üî¨ –≠—Ç–∞–ø 1: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞...")
            stage1_start = time.time()
            material_result = self.analyzer.classify_material(image_path)
            stage1_time = time.time() - stage1_start
            
            results["stages"]["material_classification"] = {
                "duration": stage1_time,
                "result": material_result
            }
            print(f"   ‚úÖ –ú–∞—Ç–µ—Ä–∏–∞–ª: {material_result['material']} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {material_result['confidence']:.2f})")
            print(f"   ‚è±Ô∏è –í—Ä–µ–º—è: {stage1_time:.2f} —Å–µ–∫")
            
            # –≠—Ç–∞–ø 2: –ê–Ω–∞–ª–∏–∑ –¥–µ—Ñ–µ–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é LLaVA
            print("üîç –≠—Ç–∞–ø 2: –ê–Ω–∞–ª–∏–∑ –¥–µ—Ñ–µ–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é LLaVA...")
            stage2_start = time.time()
            defect_analysis = self.analyzer.analyze_defects(image_path, material_result['material'])
            stage2_time = time.time() - stage2_start
            
            results["stages"]["defect_analysis"] = {
                "duration": stage2_time,
                "result": defect_analysis
            }
            print(f"   ‚úÖ –î–µ—Ñ–µ–∫—Ç—ã –Ω–∞–π–¥–µ–Ω—ã: {defect_analysis['defects_found']}")
            print(f"   ‚úÖ –¢–∏–ø—ã –¥–µ—Ñ–µ–∫—Ç–æ–≤: {defect_analysis['defect_types']}")
            print(f"   ‚úÖ –õ–æ–∫–∞—Ü–∏–∏: {defect_analysis['defect_locations']}")
            print(f"   ‚è±Ô∏è –í—Ä–µ–º—è: {stage2_time:.2f} —Å–µ–∫")
            
            # –≠—Ç–∞–ø 3: –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é SAM2
            print("üéØ –≠—Ç–∞–ø 3: –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–µ—Ñ–µ–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é SAM2...")
            stage3_start = time.time()
            segmentation_result = self.segmenter.segment_defects_with_prompts(image_path, defect_analysis)
            stage3_time = time.time() - stage3_start
            
            results["stages"]["sam2_segmentation"] = {
                "duration": stage3_time,
                "num_raw_detections": segmentation_result['num_detections']
            }
            print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {segmentation_result['num_detections']}")
            print(f"   ‚è±Ô∏è –í—Ä–µ–º—è: {stage3_time:.2f} —Å–µ–∫")
            
            # –≠—Ç–∞–ø 4: –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
            print("üé® –≠—Ç–∞–ø 4: –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞...")
            stage4_start = time.time()
            image = cv2.imread(str(image_path))
            final_masks = self.postprocessor.process_masks(
                image, 
                segmentation_result['masks'], 
                material_result['material']
            )
            stage4_time = time.time() - stage4_start
            
            results["stages"]["postprocessing"] = {
                "duration": stage4_time,
                "num_final_detections": len(final_masks)
            }
            print(f"   ‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã—Ö –¥–µ—Ñ–µ–∫—Ç–æ–≤: {len(final_masks)}")
            print(f"   ‚è±Ô∏è –í—Ä–µ–º—è: {stage4_time:.2f} —Å–µ–∫")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
            annotations = self._create_annotations(image, final_masks, material_result, defect_analysis)
            results["annotations"] = annotations
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            self._save_results(image_path, image, final_masks, results, output_path)
            
            total_time = time.time() - start_time
            print(f"\nüéâ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {total_time:.2f} —Å–µ–∫—É–Ω–¥")
            print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}")
            
            return results
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return None
    
    def _create_annotations(self, image, masks, material_result, defect_analysis):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ COCO —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ—Ç LLaVA"""
        annotations = {
            "image_info": {
                "height": image.shape[0],
                "width": image.shape[1],
                "channels": image.shape[2]
            },
            "material": material_result,
            "defect_analysis": defect_analysis,
            "defects": []
        }
        
        for i, mask in enumerate(masks):
            # –ü–æ–∏—Å–∫ –∫–æ–Ω—Ç—É—Ä–∞
            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if not contours:
                continue
            
            # –°–∞–º—ã–π –±–æ–ª—å—à–æ–π –∫–æ–Ω—Ç—É—Ä
            main_contour = max(contours, key=cv2.contourArea)
            
            # Bounding box
            x, y, w, h = cv2.boundingRect(main_contour)
            
            # –ü–ª–æ—â–∞–¥—å
            area = cv2.countNonZero(mask)
            
            # –ü–æ–ª–∏–≥–æ–Ω –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            polygon = main_contour.flatten().tolist()
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–µ—Ñ–µ–∫—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ LLaVA
            defect_type = "defect"
            object_info = None
            
            # –ü—Ä–∏–≤—è–∑–∫–∞ –∫ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –æ–±—ä–µ–∫—Ç–∞–º –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            if defect_analysis.get("objects_found") and i < len(defect_analysis["objects_found"]):
                object_info = defect_analysis["objects_found"][i]
                defect_type = object_info.get("type", "detected_object")
            elif i < len(defect_analysis.get("defect_types", [])):
                defect_type = defect_analysis["defect_types"][i]
            
            defect_annotation = {
                "id": i + 1,
                "category": defect_type,
                "bbox": [x, y, w, h],
                "area": int(area),
                "segmentation": [polygon],
                "confidence": 0.90,  # –í—ã—à–µ –±–ª–∞–≥–æ–¥–∞—Ä—è LLaVA –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º + SAM2
                "severity": defect_analysis.get("severity", "unknown"),
                "completeness": defect_analysis.get("completeness", "unknown")
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ–±—ä–µ–∫—Ç–µ –µ—Å–ª–∏ –µ—Å—Ç—å
            if object_info:
                defect_annotation.update({
                    "object_description": object_info.get("description", ""),
                    "llava_bbox": object_info.get("box", None),
                    "detection_method": "llava_coordinates"
                })
            else:
                defect_annotation["detection_method"] = "automatic_segmentation"
            
            annotations["defects"].append(defect_annotation)
        
        return annotations
    
    def _save_results(self, image_path, image, masks, results, output_path):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        image_name = Path(image_path).stem
        
        # JSON —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        json_path = output_path / f"{image_name}_analysis.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        visualization = self._create_visualization(image, masks, results)
        cv2.imwrite(str(output_path / f"{image_name}_result.jpg"), visualization)
        
        # –û—Ç–¥–µ–ª—å–Ω—ã–µ –º–∞—Å–∫–∏
        for i, mask in enumerate(masks):
            mask_path = output_path / f"{image_name}_mask_{i+1}.png"
            cv2.imwrite(str(mask_path), mask)
    
    def _create_visualization(self, image, masks, results):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å –º–∞—Å–∫–∞–º–∏ –¥–µ—Ñ–µ–∫—Ç–æ–≤"""
        vis_image = image.copy()
        
        # –¶–≤–µ—Ç–∞ –¥–ª—è –º–∞—Å–æ–∫
        colors = [
            (0, 0, 255),    # –ö—Ä–∞—Å–Ω—ã–π
            (0, 255, 0),    # –ó–µ–ª–µ–Ω—ã–π  
            (255, 0, 0),    # –°–∏–Ω–∏–π
            (0, 255, 255),  # –ñ–µ–ª—Ç—ã–π
            (255, 0, 255),  # –ü—É—Ä–ø—É—Ä–Ω—ã–π
            (255, 255, 0),  # –ì–æ–ª—É–±–æ–π
        ]
        
        # –ù–∞–ª–æ–∂–µ–Ω–∏–µ –º–∞—Å–æ–∫
        for i, mask in enumerate(masks):
            color = colors[i % len(colors)]
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–≤–µ—Ç–Ω–æ–π –º–∞—Å–∫–∏
            colored_mask = np.zeros_like(vis_image)
            colored_mask[mask > 0] = color
            
            # –ù–∞–ª–æ–∂–µ–Ω–∏–µ —Å –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å—é
            vis_image = cv2.addWeighted(vis_image, 0.7, colored_mask, 0.3, 0)
            
            # –ö–æ–Ω—Ç—É—Ä
            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            cv2.drawContours(vis_image, contours, -1, color, 2)
            
            # –ù–æ–º–µ—Ä –¥–µ—Ñ–µ–∫—Ç–∞ –∏ —Ç–∏–ø
            if contours:
                M = cv2.moments(contours[0])
                if M["m00"] != 0:
                    cx = int(M["m10"] / M["m00"])
                    cy = int(M["m01"] / M["m00"])
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø –¥–µ—Ñ–µ–∫—Ç–∞
                    defect_type = "D"
                    if "annotations" in results and i < len(results["annotations"]["defects"]):
                        defect_type = results["annotations"]["defects"][i]["category"][:3].upper()
                    
                    cv2.putText(vis_image, f"{defect_type}{i+1}", (cx-15, cy+5), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–∞—Ç–µ—Ä–∏–∞–ª–µ –∏ –∞–Ω–∞–ª–∏–∑–µ
        material_info = results["stages"]["material_classification"]["result"]
        defect_info = results["stages"]["defect_analysis"]["result"]
        
        info_text = f"Material: {material_info['material']} | Issues: {len(masks)}"
        severity_text = f"Severity: {defect_info.get('severity', 'unknown')}"
        completeness_text = f"Completeness: {defect_info.get('completeness', 'unknown')}"
        
        # –¶–≤–µ—Ç —Ç–µ–∫—Å—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏
        text_color = (255, 255, 255)  # –ë–µ–ª—ã–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if defect_info.get('severity') == 'critical':
            text_color = (0, 0, 255)  # –ö—Ä–∞—Å–Ω—ã–π
        elif defect_info.get('severity') == 'severe':
            text_color = (0, 100, 255)  # –û—Ä–∞–Ω–∂–µ–≤—ã–π
        
        cv2.putText(vis_image, info_text, (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, text_color, 2)
        cv2.putText(vis_image, severity_text, (10, 60), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 2)
        cv2.putText(vis_image, completeness_text, (10, 90), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 2)
        
        return vis_image


def main():
    parser = argparse.ArgumentParser(description="–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ –¥–µ—Ñ–µ–∫—Ç–æ–≤: LLaVA + SAM2 + OpenCV")
    parser.add_argument("--image", required=True, help="–ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    parser.add_argument("--output", default="./output", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    parser.add_argument("--model", default="standard", 
                       choices=["detailed", "standard", "latest", "onevision"],
                       help="–¢–∏–ø –º–æ–¥–µ–ª–∏ LLaVA: detailed(13B), standard(7B-–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é), latest(13B-v1.6), onevision(7B-—Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è)")
    
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–∞
    if not Path(args.image).exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.image}")
        return
    
    # –°–æ–∑–¥–∞–Ω–∏–µ pipeline —Å –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
    print(f"ü§ñ –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å: {args.model}")
    pipeline = DefectAnalysisPipeline(model_type=args.model)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
    model_info = pipeline.get_model_info()
    print(f"üìä –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_info['current_model']}")
    
    # –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞
    results = pipeline.analyze_image(args.image, args.output)
    
    if results:
        print(f"\nüìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   üî¨ –ú–∞—Ç–µ—Ä–∏–∞–ª: {results['annotations']['material']['material']}")
        print(f"   üéØ –ù–∞–π–¥–µ–Ω–æ –¥–µ—Ñ–µ–∫—Ç–æ–≤: {len(results['annotations']['defects'])}")
        print(f"   üìù –¢–∏–ø—ã –¥–µ—Ñ–µ–∫—Ç–æ–≤: {results['annotations']['defect_analysis']['defect_types']}")
        print(f"   ‚ö†Ô∏è –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å: {results['annotations']['defect_analysis']['severity']}")
        print(f"   ‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {sum(stage['duration'] for stage in results['stages'].values()):.2f} —Å–µ–∫")


if __name__ == "__main__":
    main() 